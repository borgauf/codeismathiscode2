# # -*- mode: org -*- coding: utf-8 -*-
#+TITLE: 
#+AUTHOR:
#+EMAIL: 
#+DATE: 
#+LANGUAGE:  en
# #+INFOJS_OPT: view:showall ltoc:t mouse:underline
#+HTML_HEAD: <link rel="stylesheet" href="./tufte.css" type="text/css">
#+HTML_HEAD: <link rel="stylesheet" href=".//ox-tufte.css" type="text/css">
#+HTML_HEAD_EXTRA: <style>
#+HTML_HEAD_EXTRA: article > div.org-src-container {
#+HTML_HEAD_EXTRA:     width: var(--ox-tufte-content-width);
#+HTML_HEAD_EXTRA:     max-width: var(--ox-tufte-content-width);
#+HTML_HEAD_EXTRA:     clear: none;
#+HTML_HEAD_EXTRA: }
#+HTML_HEAD_EXTRA: article > section .org-src-container {
#+HTML_HEAD_EXTRA:     width: var(--ox-tufte-src-code-width);
#+HTML_HEAD_EXTRA:     max-width: var(--ox-tufte-src-code-width);
#+HTML_HEAD_EXTRA:     clear: none;
#+HTML_HEAD_EXTRA: }
#+HTML_HEAD_EXTRA: div.org-src-container > pre { clear: none; }
#+HTML_HEAD_EXTRA: pre.example {clear: none; }
#+HTML_HEAD_EXTRA: </style>
#+INCLUDE: "./header.org" :minlevel 1
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+EXPORT_FILE_NAME: ChatWithGemini20260223Computability.html
#+OPTIONS: H:15 num:nil toc:nil \n:nil @:t ::t |:t _:{} *:t ^:{} prop:nil
#+OPTIONS: tex:t
#+OPTIONS: html-postamble:nil
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [american]
# Setup tikz package for both LaTeX and HTML export:
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{commath}
#+LaTeX_HEADER: \usepackage{pgfplots}
#+LaTeX_HEADER: \usepackage{sansmath}
#+LaTeX_HEADER: \usepackage{mathtools}
#+PROPERTY: header-args:latex+ :packages '(("" "tikz"))
#
#+PROPERTY: header-args:latex+ :exports results :fit yes
#
#+STARTUP: showall
#+STARTUP: align
#+STARTUP: indent
#+STARTUP: shrink
# This makes MathJax/LaTeX appear in buffer (UTF-8)
#+STARTUP: entitiespretty
# #+STARTUP: logdrawer # This makes pictures appear in buffer
#+STARTUP: inlineimages
# #+STARTUP: latexpreview
#+STARTUP: fnadjust
# #+OPTIONS: html-style:nil
#+html_head_extra: <style> .title { display: none; } </style>
#+html_head_extra: <style> caption.t-bottom { caption-side: bottom; } </style>

* An Introduction to Computability Theory

Computability theory, at its core, asks a deceptively simple question
from the standpoint of foundational mathematics: /What can be
mathematically calculated?/ Before physical computers existed,
mathematicians needed a rigorous, axiomatic way to define an
"effective procedure" or "algorithm."

** The Intuitive Idea vs. Formal Rigor

Intuitively, a function $f:\mathbb{N}\to\mathbb{N}$ is computable if there exists a step-by-step, mechanical set of instructions that, given an input $n$, will eventually halt and yield the correct output $f(n)$.

However, to prove that certain mathematical problems are definitively /unsolvable/ (such as Hilbert's famous /Entscheidungsproblem/), mathematicians in the 1930s like Alan Turing, Alonzo Church, Stephen Kleene, and Rózsa Péter needed to formalize exactly what a "mechanical procedure" meant.

** The Foundational Models of Computability

*** 1. Alan Turing: The Turing Machine (1936)
Turing conceptualized a theoretical machine with an infinite tape divided into discrete squares, a read/write head, and a finite set of states governed by a transition function. 



Despite its primitive nature, a Turing Machine can simulate the logic of any modern algorithm. If a Turing Machine can calculate a function, that function is considered "Turing-computable."

*** 2. Alonzo Church: The $\lambda$ Calculus (1936)
Church approached the problem purely algebraically, stripping computation down to function abstraction and application. Instead of a physical state machine, computation is the formal mathematical process of variable substitution and reduction.

Here is a conceptual example of a $\lambda$-function translated into Haskell, where we define an anonymous function that binds a variable and applies logic:

#+BEGIN_SRC haskell
-- The lambda expression \x -> x + 1
addOne :: Integer -> Integer
addOne = \x -> x + 1
#+END_SRC

*** 3. Rózsa Péter and Stephen Kleene: Recursive Functions (1930s)
Péter and Kleene focused on building complex computable functions from simple foundational axioms, relying heavily on induction. They defined /General Recursive Functions/ using three primitive, undeniably computable operations:
1. The Zero function: $Z(x)=0$
2. The Successor function: $S(x)=x+1$
3. Projection functions (selecting an argument from a tuple).

From these axioms, they used operations like composition, primitive recursion, and the \(\mu\)-operator (unbounded minimization, which mathematically acts like a =while= loop searching for a root) to construct all computable functions.

Here is how we might represent primitive recursion for addition in Scheme, building the concept of addition strictly from a successor function axiom:

#+BEGIN_SRC scheme
;; Successor function
(define (S x) (+ x 1))

;; Addition defined strictly via primitive recursion
;; Base case: x + 0 = x
;; Recursive case: x + S(y) = S(x + y)
(define (add x y)
  (if (= y 0)
      x
      (S (add x (- y 1)))))
#+END_SRC

** The Church-Turing Thesis

The miraculous discovery of foundational mathematics in the 20th century was that all these entirely distinct formalisms are perfectly equivalent in theoretical power.

A function is computable by a Turing Machine $\iff$ it is expressible in \(\lambda\)-calculus $\iff$ it is a general recursive function.

This proven mathematical equivalence led to the /Church-Turing Thesis/: the widely accepted philosophical assertion that our intuitive concept of "computability" is perfectly, wholly captured by these formal models.

** The Limits of Math: The Halting Problem

Why formalize all of this? So we can discover the absolute limits of mathematical logic.

The Halting Problem asks: Does there exist a universal program $H$ that takes the source code of any program $P$ and its input $I$, and successfully computes whether $P$ will eventually halt on $I$, or loop forever?

Turing proved that $H$ cannot exist. If it did, you could construct a paradoxical program:

#+BEGIN_SRC haskell
-- A hypothetical paradox if 'halts' were a computable function
paradox :: Program -> IO ()
paradox p = do
    if halts p p
        then loopForever
        else return ()
#+END_SRC

If =paradox= halts, it loops forever. If it loops forever, it halts. Thus, the general function =halts= cannot be computable. Because of this, we know there are well-defined mathematical truths that are permanently beyond the reach of algorithm and computation!

** Connecting Computability to Foundational Axioms

To understand how computability theory grounds itself in higher mathematics, we have to look at how Rózsa Péter and Stephen Kleene built the theory of *Recursive Functions*. Instead of relying on a mechanical metaphor like the Turing Machine, they asked: /How can we mathematically define computable functions using only the most basic, undeniable axioms of arithmetic?/

*** The Bedrock: Peano Axioms

The foundation of recursive function theory is the standard axiomatization of the natural numbers $\mathbb{N}$, specifically the Peano Axioms. In Peano arithmetic, everything is constructed from two fundamental concepts:
1. The existence of a base element: $0$.
2. The existence of a successor function: $S(n)$, which gives the "next" natural number.

Kleene and Péter took these foundational objects and elevated them to the axioms of computation. 

*** Primitive Recursion: Building from the Ground Up

A *Primitive Recursive Function* is built strictly from three initial axiomatic functions, mirroring the foundations of set theory and Peano arithmetic:

1. *The Zero Function:* $Z(x)=0$ 
2. *The Successor Function:* $S(x)=x+1$
3. *Projection Functions:* $P_{i}^{n}(x_1,\dots,x_n)=x_i$ (which fundamentally allow us to ignore variables or permute them).

From these three axioms, you can generate more complex functions using only two operations: *Composition* (chaining functions together) and *Primitive Recursion* (defining a function's value at $S(y)$ based on its value at $y$).

Because primitive recursion essentially acts as a bounded, strictly finite =for= loop, any function built this way is guaranteed to halt. Here is how we build addition and then multiplication using only these axioms in Scheme:

#+BEGIN_SRC scheme
;; The Axiomatic Initial Functions
(define (Z x) 0)
(define (S x) (+ x 1))
(define (P1-2 x y) x) ; Projection P^2_1
(define (P2-2 x y) y) ; Projection P^2_2

;; Addition: 
;; add(x, 0) = P^1_1(x) = x
;; add(x, S(y)) = S(add(x, y))
(define (add x y)
  (if (= y 0)
      x
      (S (add x (- y 1)))))

;; Multiplication:
;; mult(x, 0) = Z(x) = 0
;; mult(x, S(y)) = add(x, mult(x, y))
(define (mult x y)
  (if (= y 0)
      (Z x)
      (add x (mult x (- y 1)))))
#+END_SRC

and in Haskell

#+BEGIN_SRC haskell
-- HASKELL IMPLEMENTATION

-- The Axiomatic Initial Functions
z :: Integer -> Integer
z _ = 0

s :: Integer -> Integer
s x = x + 1

p1_2 :: Integer -> Integer -> Integer
p1_2 x _ = x

p2_2 :: Integer -> Integer -> Integer
p2_2 _ y = y

-- Addition:
-- add(x, 0) = P^1_1(x) = x
-- add(x, S(y)) = S(add(x, y))
add :: Integer -> Integer -> Integer
add x 0 = x
add x y = s (add x (y - 1))

-- Multiplication:
-- mult(x, 0) = Z(x) = 0
-- mult(x, S(y)) = add(x, mult(x, y))
mult :: Integer -> Integer -> Integer
mult x 0 = z x
mult x y = add x (mult x (y - 1))
#+END_SRC


*** Breaking the Bounds: The \(\mu\)-Operator

For a time, mathematicians wondered if *all* computable functions were primitive recursive. However, functions like the Ackermann function grow faster than any primitive recursive function can capture. More importantly, primitive recursion cannot express a procedure that searches indefinitely without a predetermined bound.

To capture the entirety of what is mathematically computable, Kleene introduced unbounded minimization: the *\(\mu\)-operator*.

Given a function $f(x, y)$, the \(\mu\)-operator searches for the
smallest $y$ such that $f(x, y)=0$. We write this formally as: $$\mu y
[f(x, y) = 0]$$

[Image illustrating unbounded minimization search mapping inputs to the lowest root in a mathematical function]

This is the mathematical equivalent of a =while= loop. It evaluates $f(x, 0)$, then $f(x, 1)$, then $f(x, 2)$, and so on, until it hits $0$. 

*Here is the profound catch that links to the limits of mathematics:* If no such $y$ exists, the \(\mu\)-operator searches forever. The introduction of the \(\mu\)-operator elevates primitive recursive functions to *General Recursive Functions* (also known as \(\mu\)-recursive functions), but it destroys the guarantee of termination. It introduces *partial functions* into our foundational logic—functions that are well-defined in their construction but undefined for certain inputs because they loop infinitely. This is the exact algebraic origin of the Halting Problem!

Here is the \(\mu\)-operator modeled in Haskell. Notice how it inherently risks an infinite loop if a root does not exist:

#+BEGIN_SRC haskell
-- The mu-operator takes a function f(x, y) and an input x.
-- It searches for the smallest y starting from 0.
mu :: (Integer -> Integer -> Integer) -> Integer -> Integer
mu f x = search 0
  where
    search y
      | f x y == 0 = y
      | otherwise  = search (y + 1)

-- Example: Finding integer division (x / 2) by searching for y where 2*y - x = 0
-- (Assumes x is even for this simple partial function)
f_div2 :: Integer -> Integer -> Integer
f_div2 x y = abs ((2 * y) - x) 

-- mu f_div2 10  => returns 5
-- mu f_div2 11  => LOOPS FOREVER (partial function)
#+END_SRC

*** The \(\lambda\)-Calculus Equivalent: Church Numerals

Alonzo Church bypassed the Peano axioms entirely by encoding the axioms of arithmetic directly into function abstractions. In the $\lambda$-calculus, numbers do not exist as primitive objects. Instead, a number $n$ is defined by /how many times/ a function is applied to an argument. These are called *Church Numerals*:

- $0 \equiv \lambda f. \lambda x. x$ (Apply the function $f$ zero times to $x$)
- $1 \equiv \lambda f. \lambda x. f x$ (Apply $f$ one time to $x$)
- $2 \equiv \lambda f. \lambda x. f (f x)$ 

To get general recursion (the equivalent of the $\mu$-operator) in
\(\lambda\)-calculus, Church utilized *Fixed-Point Combinators* (like
the Y-combinator), which allow a function to self-replicate and pass
itself as an argument, enabling infinite loops without naming the
function.

** Combinators, the Y-Combinator, and Theorem Provers

To move from the mechanical intuition of Turing to the pure, algebraic abstraction of Church, we must look at how computation can exist purely as the substitution of symbols. This brings us to combinatory logic, the Y-Combinator, and how modern type theory deals with the inherent danger of infinite recursion.

*** Introduction to Combinators

In the \(\lambda\)-calculus, a *combinator* is strictly defined as a lambda expression with absolutely *no free variables*. Every variable used in the body of the function must be bound by a \(\lambda\) in the function's parameter list. 

Combinators act as pure, self-contained mathematical building blocks. They depend on zero outside context or global state. 

Historically, Moses Schönfinkel and Haskell Curry developed *Combinatory Logic* independently of Church, proving that you do not even need variables to perform universal computation—you only need a few primitive combinators. The most famous set is the SKI combinator calculus, built from three fundamental functions:

1. *The Identity Combinator (I):* Returns its argument.
   $I = \lambda x. x$
2. *The Constant Combinator (K):* Takes two arguments, returns the first, and discards the second.
   $K = \lambda x. \lambda y. x$
3. *The Substitution Combinator (S):* Applies one function to another, passing a shared argument to both.
   $S = \lambda x. \lambda y. \lambda z. x z (y z)$

Remarkably, any computable function can be expressed purely as a tree of S and K combinators!



Here are the SKI combinators expressed in Haskell:

#+BEGIN_SRC haskell
-- The SKI Combinators in Haskell

-- I: Identity
i :: a -> a
i x = x

-- K: Constant (similar to Haskell's built-in 'const')
k :: a -> b -> a
k x y = x

-- S: Substitution
s :: (a -> b -> c) -> (a -> b) -> a -> c
s x y z = (x z) (y z)
#+END_SRC

*** The Y-Combinator: Achieving General Recursion

In our previous discussion, we saw that primitive recursion halts, while the \(\mu\)-operator introduces the potential for infinite loops (general recursion). In the pure \(\lambda\)-calculus, anonymous functions have no names, so they cannot call themselves. How, then, do we achieve general recursion?

We use a *Fixed-Point Combinator*. In mathematics, a fixed point of a function $f$ is a value $x$ such that $f(x) = x$. A fixed-point combinator is a higher-order function $Y$ that finds the fixed point of *any* function passed to it.

Mathematically:
$Y f = f (Y f)$

By repeatedly expanding $Y f$ into $f (f (Y f))$, we achieve unbounded recursion without ever explicitly naming the function! 

The most famous of these is the *Y-Combinator*, discovered by Haskell Curry:
$$Y = \lambda f. (\lambda x. f (x x)) (\lambda x. f (x x))$$



If we want to write a recursive function like factorial in Scheme without using =define= to give it a name, we use the Y-Combinator to pass the function into itself. 

#+BEGIN_SRC scheme
;; The strict-evaluation Y-Combinator (often called the Z-Combinator in Scheme 
;; to prevent infinite expansion before application due to applicative order)
(define Y
  (lambda (f)
    ((lambda (x) (x x))
     (lambda (x)
       (f (lambda (v) ((x x) v)))))))

;; A "pseudo-recursive" factorial function that does not call itself by name.
;; Instead, it expects a function 'recur' to be passed to it.
(define proto-factorial
  (lambda (recur)
    (lambda (n)
      (if (= n 0)
          1
          (* n (recur (- n 1)))))))

;; We achieve actual recursion by applying the Y-combinator!
(define factorial (Y proto-factorial))

;; (factorial 5) => 120
#+END_SRC

*** Theorem Provers and the Halting Guarantee

This brings us to a beautiful intersection of computability theory and modern foundational mathematics: formal theorem proving. 

In the Curry-Howard correspondence, mathematical proofs are equivalent to computer programs, and mathematical propositions (theorems) are equivalent to types. If you can write a program that evaluates to a specific type, you have proven the corresponding theorem.

However, the Y-combinator and the \(\mu\)-operator introduce a catastrophic problem for logic: *infinite loops*. 

If a system allows unbounded, non-terminating recursion, you can write a function that loops forever and assign it the type =False= (a mathematical contradiction). This means an infinitely looping program effectively acts as a "proof" of anything, destroying the consistency of the entire logical system! 

Because of this, modern proof assistants and theorem provers like Coq, Lean 4, Agda, and Isabelle strictly forbid general recursion by default. 

To maintain logical soundness, these systems require all functions to be *total*—meaning they are mathematically guaranteed to halt on all valid inputs. They achieve this by enforcing rules very similar to the Peano/Primitive Recursion axioms we explored earlier:

1. *Structural Recursion:* A recursive call is only valid if it operates on a strictly smaller, structurally "sub-component" of the original input. For example, stripping one layer of $S(x)$ off a Peano number.
2. *Well-Founded Relations:* If a function is not trivially structurally recursive, the mathematician must supply a mathematically rigorous proof that the inputs decrease according to some well-founded relation (a sequence that cannot decrease infinitely).

By essentially banning the \(\mu\)-operator and the unbounded Y-Combinator, theorem provers sacrifice Turing-completeness at the programmatic level to gain absolute, bulletproof certainty at the logical level.
