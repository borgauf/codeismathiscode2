# # -*- mode: org -*- coding: utf-8 -*-
#+TITLE: 
#+AUTHOR:
#+EMAIL: 
#+DATE: 
#+LANGUAGE:  en
# #+INFOJS_OPT: view:showall ltoc:t mouse:underline
#+HTML_HEAD: <link rel="stylesheet" href="./tufte.css" type="text/css">
#+HTML_HEAD: <link rel="stylesheet" href=".//ox-tufte.css" type="text/css">
#+HTML_HEAD_EXTRA: <style>
#+HTML_HEAD_EXTRA: article > div.org-src-container {
#+HTML_HEAD_EXTRA:     width: var(--ox-tufte-content-width);
#+HTML_HEAD_EXTRA:     max-width: var(--ox-tufte-content-width);
#+HTML_HEAD_EXTRA:     clear: none;
#+HTML_HEAD_EXTRA: }
#+HTML_HEAD_EXTRA: article > section .org-src-container {
#+HTML_HEAD_EXTRA:     width: var(--ox-tufte-src-code-width);
#+HTML_HEAD_EXTRA:     max-width: var(--ox-tufte-src-code-width);
#+HTML_HEAD_EXTRA:     clear: none;
#+HTML_HEAD_EXTRA: }
#+HTML_HEAD_EXTRA: div.org-src-container > pre { clear: none; }
#+HTML_HEAD_EXTRA: pre.example {clear: none; }
#+HTML_HEAD_EXTRA: </style>
#+INCLUDE: "./header.org" :minlevel 1
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+EXPORT_FILE_NAME: ChatWithGemini20260223Tokens.html
#+OPTIONS: H:15 num:nil toc:nil \n:nil @:t ::t |:t _:{} *:t ^:{} prop:nil
#+OPTIONS: tex:t
#+OPTIONS: html-postamble:nil
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [american]
# Setup tikz package for both LaTeX and HTML export:
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{commath}
#+LaTeX_HEADER: \usepackage{pgfplots}
#+LaTeX_HEADER: \usepackage{sansmath}
#+LaTeX_HEADER: \usepackage{mathtools}
#+PROPERTY: header-args:latex+ :packages '(("" "tikz"))
#
#+PROPERTY: header-args:latex+ :exports results :fit yes
#
#+STARTUP: showall
#+STARTUP: align
#+STARTUP: indent
#+STARTUP: shrink
# This makes MathJax/LaTeX appear in buffer (UTF-8)
#+STARTUP: entitiespretty
# #+STARTUP: logdrawer # This makes pictures appear in buffer
#+STARTUP: inlineimages
# #+STARTUP: latexpreview
#+STARTUP: fnadjust
# #+OPTIONS: html-style:nil
#+html_head_extra: <style> .title { display: none; } </style>
#+html_head_extra: <style> caption.t-bottom { caption-side: bottom; } </style>


* What is a Token in AI and Machine Learning?

In the world of AI, models do not "read" words the way humans do. Instead, they process text by breaking it down into discrete units called **tokens**. 

---

** How Tokenization Works
Tokenization is the process of converting a raw string of text into a sequence of integers. This happens in three primary stages:

1. **Splitting:** The text is divided into pieces. Depending on the algorithm, a token could be a whole word, a part of a word (sub-word), or even a single character.
2. **Mapping:** Each unique token is assigned a specific number (index) from a pre-defined **vocabulary**.
3. **Encoding:** The text "Hello world" might become a sequence of integers like $[15496, 995]$.

---

** Common Tokenization Methods


| Method              | Level                    | Example: "Unhappiness"                                    |
| :---                | :---                     | :---                                                      |
| **Word-level**      | Each word is a token     | ~["Unhappiness"]~                                         |
| **Character-level** | Each letter is a token   | ~["U", "n", "h", "a", "p", "p", "i", "n", "e", "s", "s"]~ |
| **Sub-word**        | Common chunks are tokens | ~["Un", "happi", "ness"]~                                 |

*Note:* Most modern Large Language Models (LLMs) use **Sub-word Tokenization** (like Byte-Pair Encoding) because it efficiently handles rare words and prefixes/suffixes.

---

** The Mathematical Representation
Once text is tokenized, it is converted into a numerical format that the machine can manipulate.

1. **Input Indices:** A sequence of tokens is represented as a vector $X$:
   $$X = [t_1, t_2, t_3, \dots, t_n]$$
   where each $t_i$ is an integer index.

2. **Embeddings:** Each token index is then looked up in an **Embedding Matrix** $E$. This transforms the integer into a high-dimensional vector of floating-point numbers:
   $$v_i = E(t_i)$$
   where $v_i \in \mathbb{R}^d$ and $d$ is the dimensionality of the model (e.g., $d = 768$ or $d = 4096$).

---

** Why Tokens Matter
- **Context Window:** AI models have a limit on how many tokens they can process at once (e.g., 8k, 32k, or 128k tokens).
- **Cost:** Many API providers (like OpenAI or Anthropic) charge users based on the number of tokens processed, not the number of words.
- **Efficiency:** On average, $1000$ tokens is roughly equal to $750$ English words.

#+BEGIN_QUOTE
**Summary:** A token is the bridge between human-readable text and the mathematical vectors that a neural network operates on.
#+END_QUOTE

* Practical Example: Sub-word Tokenization

In modern AI, we use **Sub-word Tokenization**. This allows the model to understand the root of a word while also recognizing its suffixes or prefixes.

---

** The Breakdown: "Tokenization is amazing!"

When a modern LLM processes this sentence, it doesn't see three words. It breaks them into pieces based on frequency and logic:


| Original Segment | Token Produced | Type              |
| :---             | :---           | :---              |
| "Token"          | ~Token~        | Root Word         |
| "ization"        | ~ization~      | Suffix            |
| " is"            | ~_is~          | Word (with space) |
| " amazing"       | ~_amazing~     | Word (with space) |
| "!"              | ~!~            | Punctuation       |

**Note:** The "_" symbol represents a leading space, which is often treated as part of the token itself.

---

** Mathematical Mapping to IDs

Each of these strings corresponds to a unique integer in the model's vocabulary ($V$). Let's assume a simplified vocabulary where:

- $t_{345} = \text{"Token"}$
- $t_{1201} = \text{"ization"}$
- $t_{312} = \text{" is"}$
- $t_{4598} = \text{" amazing"}$
- $t_{0} = \text{"!"}$

The sentence is converted into a vector $X$:
$$X = [345, 1201, 312, 4598, 0]$$

---

** Why this is Efficient

1. **Vocabulary Size:** By using sub-words, the model doesn't need a separate entry for "Token," "Tokenize," "Tokenizing," and "Tokenization." It just needs $Token$ + $suffix$.
2. **Handling Unknowns:** Even if the model has never seen a specific complex word, it can usually break it down into smaller characters or chunks it *does* know, preventing the "Unknown Word" error common in older AI.

#+BEGIN_QUOTE
**Visual Tip:** If you ever want to see this live, you can visit the **OpenAI Tokenizer** web tool. It highlights the text in different colors to show exactly where one token ends and the next begins.
#+END_QUOTE

* Byte-Pair Encoding (BPE) Explained

Byte-Pair Encoding is an iterative algorithm that starts with individual characters and merges the most frequently occurring adjacent pairs into new, single tokens.

---

** The BPE Algorithm Process

Suppose we have a tiny "corpus" (dataset) with the words:
- "low": 5 times
- "lower": 2 times
- "newest": 6 times
- "widest": 3 times

1. **Step 1: Initialize Vocabulary**
   Every character is treated as a token:
   $$V = \{l, o, w, e, r, n, s, t, i, d\}$$

2. **Step 2: Count Adjacent Pairs**
   The algorithm looks for the pair of tokens that appear next to each other most often. In our example, "e" and "s" appear together in both "newest" and "widest" ($6 + 3 = 9$ times).

3. **Step 3: Merge the Pair**
   A new token "es" is created and added to the vocabulary.
   $$V = V \cup \{es\}$$

4. **Step 4: Repeat**
   Next, it might find that "es" and "t" appear together 9 times. It merges them into "est".
   $$V = V \cup \{est\}$$

---

** The Logic of "Greedy" Merging

BPE is a **greedy algorithm**. It identifies patterns by calculating the frequency $f$ of pairs $(a, b)$:

$$ \text{pair}_{next} = \arg\max_{a,b \in V} f(a, b) $$

By repeating this $k$ times (where $k$ is a hyperparameter), the model builds a vocabulary of size $S$. Large models typically use a vocabulary size $S \approx 50,000$ to $100,000$.

---

** Advantages of BPE


| Advantage                  | Description                                                               |
| :---                       | :---                                                                      |
| **No "Out-of-Vocabulary"** | Since it starts with characters, any word can be broken down into pieces. |
| **Compression**            | Common words like "the" become single tokens, while rare words are split. |
| **Morphological Learning** | It naturally learns that "ing", "ed", and "tion" are meaningful units.    |

#+BEGIN_QUOTE
**Summary:** BPE allows the machine to "decide" its own alphabet based on the statistical distribution of the data it sees, rather than humans forcing a dictionary upon it.
#+END_QUOTE

* The Attention Mechanism: Connecting the Tokens

Once a sentence is converted into tokens and then into vectors (embeddings), the model needs to understand how those tokens relate to one another. It does this through **Self-Attention**.

---

** The Goal of Attention
In the sentence /"The bank of the river,"/ the token **"bank"** means something different than in /"The bank gave a loan."/ 

Self-attention allows the model to "attend" to surrounding tokens to resolve meaning. To calculate this, the model creates three new vectors for every token $i$:
1. **Query ($Q_i$):** What am I looking for?
2. **Key ($K_i$):** What information do I contain?
3. **Value ($V_i$):** What information do I share if I am relevant?

---

** The Mathematical Formula

The relationship between all tokens in a sequence is calculated using the **Scaled Dot-Product Attention** formula:

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

Where:
- $Q, K, V$ are matrices representing the Queries, Keys, and Values of all tokens.
- $QK^T$ calculates the "score" or similarity between every token and every other token.
- $\sqrt{d_k}$ is a scaling factor based on the dimension of the keys to keep gradients stable.
- The **softmax** function ensures that the attention scores for a single token sum up to $1$ (or $100\%$).

---

** Visualizing the Relationship (Attention Map)

For the sentence "The animal didn't cross the street because it was too tired":


| Token     | High Attention To... | Why?                                              |
| :---      | :---                 | :---                                              |
| **it**    | "animal"             | The model identifies the pronoun referent.        |
| **tired** | "animal"             | It links the state of being tired to the subject. |
| **cross** | "street"             | It understands the action-object relationship.    |

---

** Why "Multi-Head" Attention?

Modern models use **Multi-Head Attention**. Instead of one attention calculation, they run several (e.g., 8, 12, or 16) in parallel. 
- **Head 1** might focus on grammar and syntax.
- **Head 2** might focus on pronoun resolution.
- **Head 3** might focus on emotional tone.

#+BEGIN_QUOTE
**Summary:** If tokens are the "words," Attention is the "logic" that weaves them into a coherent context, allowing the model to understand complex relationships rather than just seeing a list of symbols.
#+END_QUOTE

* The Softmax Function: Turning Scores into Probabilities

In the Attention mechanism, the raw dot-product of $Q$ and $K$ produces a set of scores that can be any real number (large positive, zero, or negative). To make these scores useful, we apply **Softmax**.

---

** The Mathematical Definition

For a vector of raw scores $z$ (often called "logits"), the Softmax value for the $i$-th element is calculated as:

$$ \sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} $$

Where:
- $e^{z_i}$ is the exponential function applied to the input score.
- $\sum_{j=1}^{K} e^{z_j}$ is the sum of exponents of all scores in the vector.
- $K$ is the total number of tokens being compared.

---

** Why use Softmax?

1. **Probability Distribution:** The resulting values are always between $0$ and $1$, and they sum exactly to $1.0$ (or $100\%$).
2. **Amplification:** Because it uses the exponential $e$, it naturally exaggerates the largest score. This helps the model "focus" its attention on the most relevant token while suppressing noise.
3. **Differentiability:** It is a smooth, continuous function, which allows the model to learn using calculus (backpropagation).

---

** Numerical Example

Imagine a token is looking at three other tokens, and the raw attention scores ($QK^T$) are:
- Token A: $2.0$
- Token B: $1.0$
- Token C: $0.1$


| Token     | Raw Score ($z$) | Exponent ($e^z$) | Softmax Output ($\sigma$) |
| :---      | :---            | :---             | :---                 |
| **A**     | $2.0$           | $7.39$           | **~0.66 (66%)**      |
| **B**     | $1.0$           | $2.72$           | **~0.24 (24%)**      |
| **C**     | $0.1$           | $1.11$           | **~0.10 (10%)**      |
| **Total** |                 | $11.22$          | **1.00 (100%)**      |

---

** The Temperature Parameter ($T$)

Sometimes, researchers add a "Temperature" variable to the Softmax:
$$ \sigma(z)_i = \frac{e^{z_i / T}}{\sum e^{z_j / T}} $$

- **Low $T$ (e.g., $0.1$):** Makes the distribution "sharper." The model becomes very confident and picks only the top choice.
- **High $T$ (e.g., $1.5$):** Makes the distribution "flatter." The model becomes more creative and likely to pick less probable tokens.

#+BEGIN_QUOTE
**Summary:** Softmax is the filter that tells the model exactly how much "weight" to give to each piece of information when calculating the final representation of a token.
#+END_QUOTE

* Next Token Prediction: The Generative Loop

Large Language Models (LLMs) are essentially sophisticated "next-word predictors." They generate text one token at a time, using the previous tokens as the context for the next calculation.

---

** The Prediction Pipeline

After the **Attention** layers and **Softmax** have processed the input, the final hidden state for the last token is passed through a "Linear Header." This maps the high-dimensional vector back into the size of the original vocabulary.

1. **Logits Generation:** The model produces a vector $Z$ the size of the entire vocabulary (e.g., 50,257 for GPT-2). Each number represents the "likelihood" of that specific token being next.
2. **Probability Mapping:** Softmax is applied to $Z$ to create a probability distribution $P$:
   $$P(t_{n+1} | t_1, \dots, t_n) = \text{softmax}(Z)$$
3. **Sampling:** The model chooses the next token $t_{n+1}$ based on these probabilities.

---

** Strategy: How to "Pick" the Next Token

The way a model chooses from the probability distribution is called a **Decoding Strategy**:


| Strategy            | Logic                                                                          | Result                              |
| :---                | :---                                                                           | :---                                |
| **Greedy Search**   | Always pick the token with the highest probability.                            | Predictable, but can be repetitive. |
| **Top-K Sampling**  | Only consider the top $K$ most likely tokens.                                  | Balances logic and variety.         |
| **Top-P (Nucleus)** | Pick from the smallest set of tokens whose cumulative probability exceeds $P$. | Dynamic and very human-like.        |

---

** The Autoregressive Loop

The "magic" of AI happens because this process is recursive. Once a token is predicted, it is appended to the input, and the whole cycle starts again:

$$ \text{Step 1: } [\text{The}] \rightarrow \text{cat} $$
$$ \text{Step 2: } [\text{The, cat}] \rightarrow \text{sat} $$
$$ \text{Step 3: } [\text{The, cat, sat}] \rightarrow \text{on} $$

This continues until the model predicts a special **End of Text (EOS)** token, signaling that the response is complete.

---

** Visualizing the Probabilities

For the phrase "The weather is today...", the model might see:

- **"sunny"**: $0.45$
- **"cloudy"**: $0.25$
- **"beautiful"**: $0.15$
- **"purple"**: $0.0001$

The model then "rolls the dice" (influenced by your **Temperature** setting) to select which one to print to your screen.

#+BEGIN_QUOTE
**Summary:** Generative AI is a continuous loop of calculating probabilities and selecting symbols. It doesn't "know" the whole sentence in advance; it discovers it one token at a time.
#+END_QUOTE
