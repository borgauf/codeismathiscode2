# # -*- mode: org -*- coding: utf-8 -*-
#+TITLE: 
#+AUTHOR:
#+EMAIL: 
#+DATE: 
#+LANGUAGE:  en
# #+INFOJS_OPT: view:showall ltoc:t mouse:underline
#+HTML_HEAD: <link rel="stylesheet" href="./tufte.css" type="text/css">
#+HTML_HEAD: <link rel="stylesheet" href=".//ox-tufte.css" type="text/css">
#+HTML_HEAD_EXTRA: <style>
#+HTML_HEAD_EXTRA: article > div.org-src-container {
#+HTML_HEAD_EXTRA:     width: var(--ox-tufte-content-width);
#+HTML_HEAD_EXTRA:     max-width: var(--ox-tufte-content-width);
#+HTML_HEAD_EXTRA:     clear: none;
#+HTML_HEAD_EXTRA: }
#+HTML_HEAD_EXTRA: article > section .org-src-container {
#+HTML_HEAD_EXTRA:     width: var(--ox-tufte-src-code-width);
#+HTML_HEAD_EXTRA:     max-width: var(--ox-tufte-src-code-width);
#+HTML_HEAD_EXTRA:     clear: none;
#+HTML_HEAD_EXTRA: }
#+HTML_HEAD_EXTRA: div.org-src-container > pre { clear: none; }
#+HTML_HEAD_EXTRA: pre.example {clear: none; }
#+HTML_HEAD_EXTRA: </style>
#+INCLUDE: "./header.org" :minlevel 1
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+EXPORT_FILE_NAME: ComputingInContext1.html
#+OPTIONS: H:15 num:nil toc:nil \n:nil @:t ::t |:t _:{} *:t ^:{} prop:nil
#+OPTIONS: tex:t
#+OPTIONS: html-postamble:nil
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [american]
# Setup tikz package for both LaTeX and HTML export:
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{commath}
#+LaTeX_HEADER: \usepackage{pgfplots}
#+LaTeX_HEADER: \usepackage{sansmath}
#+LaTeX_HEADER: \usepackage{mathtools}
#+PROPERTY: header-args:latex+ :packages '(("" "tikz"))
#
#+PROPERTY: header-args:latex+ :exports results :fit yes
#
#+STARTUP: showall
#+STARTUP: align
#+STARTUP: indent
#+STARTUP: shrink
# This makes MathJax/LaTeX appear in buffer (UTF-8)
#+STARTUP: entitiespretty
# #+STARTUP: logdrawer # This makes pictures appear in buffer
#+STARTUP: inlineimages
# #+STARTUP: latexpreview
#+STARTUP: fnadjust
# #+OPTIONS: html-style:nil
#+html_head_extra: <style> .title { display: none; } </style>
#+html_head_extra: <style> caption.t-bottom { caption-side: bottom; } </style>

* Computing in Context 1

\\
#+begin_figure
#+CAPTION: George Boole bust @ University College Cork.
[[file:images/BooleBustUCCorkSepia.png]]
#+end_figure


** Bibliography :noexport:
:PROPERTIES:
:header-args: :dir "/home/galaxybeing/Dropbox/org/codeismathiscode2"
:END:
:RESOURCES:
- [[bibliography:~/Dropbox/org/biblio/ref.bib][Bibliography]]
- [[cite:&friedman1995little]]
:END:


* 

** Axiomatic mathematics for /operationalization/

#+begin_quote
Young man, in mathematics you don't understand things. You just get
used to them. \\
--- John von Neumann
#+end_quote


Often with deep math texts the first chapter is devoted to a sort of
"overture," a 30,000-foot view of what's to come in the rest of the
book. So it will be with this, our intro chapter where we'll explore
just how odd, different, /crazy/ so-call higher math can be. The main
difference between K-12 math and higher math is just how strange and
philosophical the latter can get. But why do we have to worry about
higher math in the first place? As we've discussed in the Preface, the
theory that grew up around the development of digital computers has
always been rigorous higher-math-based. /It had to be./ And yes, our
catchphrase, /formalize to operationalize/ is central to everything.

What follows is an admittedly odd sort of dive into how to upgrade
your understanding of algebra, how to lift it out of the mirky,
intuitive, seat-of-pants methods you've learned so far and move it
into more systematic, rule-based procedures. Again, why?  Because,
ultimately, the whole subject of computability in general and computer
operation specifically must be grounded in logical entailment. Higher
math offers this packaging and handling of logic and its
consequences. How higher math "hangs together" can be a marvel, but
just as often a frustrating descent into confusion---at least until
the next plateau, your next eureka moment lands. What follows in this,
our overture, omnibus intro chapter will seem like a great challenge
to many; but in the spirit of math text first chapters, /do not
panic/, do not take this too seriously, do not attempt to rote learn
or memorize; rather, just go along for the ride. Higher math is all
about creating elegant, self-contained, self-referential, soup-to-nuts
explanations that scale, scale, scale. Reach a higher plateau. See
much farther. Higher math always seeks to /generalize/. Why?  Because
there is a firmer grasp to be had from developing tight,
all-encompassing systems of symbolism and generalization over a
subject. Recall with Einstein's General Relativity that gravity, a
real-world phenomenon, is actually an /artefact/ of a containing
mathematical theory! That's an incredible power we humans have gained
over the universe and reality.[fn:1]

Let us begin gently by exploring that odd-seeming request math
teachers begin asking of their beginning algebra students when solving
homework problems: "Show your work." Mysteriously, teachers suddenly
want to see all the /steps/ their students took to arrive at final
answers. Why? By listing every step we demonstrate more clearly our
process of solving a problem. And if we didn't get the right answer,
we can better find where we took a wrong turn. Showing all steps is
about moving towards a solution in a verifiable way and leaving behind
intuition, conditioned instincts, hunches, and guesswork. Let's take
"show your steps" further by involving the foundational **rules** or
**[[https://en.wikipedia.org/wiki/Axiom][axioms]]**[fn:2] of algebra as seen from the higher-math
perspective. Some of these may be familiar, others not, and much will
seem---odd---as in, Why do I need to worry about this stuff?
Throughout /Computing in Context/ we emphasize /formalize to
operationalize/, and this is where we'll start. Below are /properties/
we can consider axioms. These can be found in this basic form in many
textbooks. Peruse, then we'll put them to use.

@@html:<font color = "#0d3db3">@@
**Properties of Arithmetic**

For any numbers $a$, $b$, and $c$:

- **Axiom 1**: /Commutative properties---addition, multiplication/ \\
  $a + b = b + a\;; \quad ab = ba$
- **Axiom 2**: /Associative properties---addition, multiplication/ \\
  $(a + b) + c = a + (b + c)\;; \quad (ab)c = a(bc)$
- **Axiom 3**: /Distributive property/ \\
  $a(b + c) = ab + ac$
- **Axiom 4**: /Existence of identity elements/ \\
  There exist two distinct numbers, $0$ and $1$, such that for every
  $a$ we have $a + 0 = a$ and $a \cdot 1 = a$
- **Axiom 5**: /Existence of negatives/ \\  
  For every number $a$ there is a number $b$ such that $a + b = 0$
- **Axiom 6**: /Existence of reciprocals/ \\  
  For every number $a \ne 0$ there is a number $b$ such that $ab = 1$,
  e.g., $b = \frac{1}{a}$

**Properties of Equality**

For any numbers $a$, $b$, $c$, and $d$:

- /Reflexive property/ \\
  $a = a$ ...any number is equal to itself.
- /Transitive property/ \\
  If $a = b$ and $b = c$, then $a = c$ ...this property often takes the
  form of the substitution property, which says that if $b = c$, you can
  substitute $c$ for $b$. 
- /Symmetric property/ \\
  If $a = b$, then $b = a$
- /Addition property/ \\
  If $a = b$, then $a + c = b + c$ ...also, if $a = b$ and
  $c = d$, then $a + c = b + d$
- /Subtraction property/ \\
  If $a = b$, then $a ‚àí c = b ‚àí c$ ...also, if $a = b$ and $c = d$, then $a ‚àí c = b ‚àí d$
- /Multiplication property/ \\
  If $a = b$, then $ac = bc$ ...also, if $a = b$ and $c = d$, then $ac = bd$
- /Division property/ \\
  If $a = b$, then $\frac{a}{c} = \frac{b}{c}$ provided $c \ne 0$
- /Square root property/ \\
  If $a^2 = b$, then $a = \pm\sqrt{b}$
- /Zero product property/ \\
  If $ab = 0$, then $a = 0$ or $b = 0$ or both $a$ and $b = 0$
@@html:</font>@@

You may have seen some or all of these before and wondered what role it
actually played. So let's solve an algebra expression for $x$, adding
in /justifications/ from the properties above for each step[fn:3]

‚åú\\
ùñüùï≠: Solve $5x - 12 = 3(x + 2)$ for $x$: 
\begin{align*}
5x - 12 &= 3(x + 2) \quad \quad &&\text{Given} \\
5x - 12 &= 3x +6 \quad \quad &&\text{Distributive Property} \\
5x &= 3x + 18 \quad \quad &&\text{Addition property of equality} \\
2x &= 18 \quad \quad &&\text{Subtraction property of equality} \\
x &= 9 \quad \quad &&\text{Division property of equality} \\
\end{align*}
‚åü \\

All right, we've shown our work, step-by-step, but we also explained,
/justified/ each **rewrite**[fn:4] of each step by attaching one of
the rules above --- /Distributive Law, Addition property of equality/,
etc., to each line. Now, it's highly doubtful any middle or high
school algebra teacher discussed algebra in terms of axioms or asked
you to go into such depth. But as mentioned above, /from these axioms
we can derive all algebraic operations/. Here we begin to take algebra
out of conditioned, rote response and **operationalize** it.

So far so strange... Typically, when you were first learning algebra
there was no talk about such formal, systematic methods. Instead, you
watched and followed what the teacher was doing, picked up on, made
sense of the seemingly impromptu rules, looking for patterns and clues
in the examples ... in effect, you learned in an instinctive,
pattern-recognition way to solve problems after much imitation and
repetition. You /internalized/ the methods without going into any
further depth. However, starting at the university, you will be
required to grasp things at a more systematic level. You'll be
introduced to the greater world of **[[https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)][mathematical formalism]]**.

With axiomatic mathematics we start with a set of axioms, i.e.,
givens, rules, laws, statements to be taken as the most basic,
atomic[fn:5] facts, then employ **[[https://en.wikipedia.org/wiki/Deductive_reasoning][deductive reasoning]]** to /prove/
further statements or **[[https://en.wikipedia.org/wiki/Theorem][theorems]]**. Taken together, a set of proven
theorems becomes the body of a particular branch of math. The
axiom-to-theorems approach is diametrically opposed to typical K-12
"muscle-memory" math where methods aren't described as theorems;
instead, the mantra is "Do enough problems until you get the hang of
it." Now, however, the goal changes from "get problems solved" to
attaining a higher, more systematic level of understanding, one where
we will eventually be able to go out into the world and create new
mathematics. Whether or not you've seen axiomatic systems directly
discussed, real-world STEM often relies on this formalistic approach,
even when it's not obvious. From now on we'll be cognizant of, mindful
of the math formalism, mainly to prepare us for what we'll face in
higher education settings. And hopefully along the way we'll begin to
see the importance and necessity of axiomatic mathematics.

So real mathematics is groupings of theorems that have been /proven/
based on axioms. Here's Kenneth Rosen from his /[[https://www.mheducation.com/highered/product/Discrete-Mathematics-and-Its-Applications-Rosen.html][Discrete Mathematics
and Its Applications]]/ a widely-used college textbook in computer
science departments where he emphasizes **[[https://en.wikipedia.org/wiki/Mathematical_proof][proof]]** [fn:6]

#+begin_quote
To understand mathematics, we must understand what makes up a correct
mathematical argument, that is, a **proof**. Once we /prove/ a
mathematical statement is true, we call it a **theorem**. A collection
of theorems on a topic organize [into] what we know about this topic.
#+end_quote

Let's take a quick /math holiday/[fn:7] to give an example of how
proof---in this case the lack thereof---plays an important role in
real-world math and computers.

‚á≤ @@html:<font color = "#0d3db3">@@ The **Collatz conjecture** is one
of the most famous "unsolved problems" in mathematics. Also known as
the $3n + 1$ problem, it is deceptively simple, i.e., a child can
understand the rules, yet the world‚Äôs greatest mathematicians have
failed to /prove/ it will always terminate and not just keep
running. The /Collatz conjecture/ says...

...starting with any positive integer $n$, if we apply these two steps to
some number $n$ repeatedly, we should eventually wind up at the number
$1$:

- If $n$ is even: Divide it by $2$ (\(n/2\)).
- If $n$ is odd: Multiply it by $3$ and add $1$ (\(3n+1\)).
@@html:</font>@@

‚åú\\
ùñüùï≠: Starting with n = 7

1. $7$ is odd: \((3 \cdot 7)+1=22\)
2. $22$ is even: \(22/2=11\)
3. $11$ is odd: \((3 \cdot 11)+1=34\)
4. $34$ is even: \(34/2=17\)
5. $17$ is odd: \((3 \cdot 17)+1=52\)
6. $52$ is even: \(52/2=26\)
7. $26$ is even: \(26/2=13\)
8. $13$ is odd: \((3 \cdot 13)+1=40\)
9. $40$ is even: \(40/2=20\)
10. $20$ is even: \(20/2=10\)
11. $10$ is even: \(10/2=5\)
12. $5$ is odd: \((3 \cdot 5)+1=16\)
13. $16$ is even: \(16/2=8\)
14. $8$ is even: \(8/2=4\)
15. $4$ is even: \(4/2=2\)
16. $2$ is even: \(2/2=1\) (Target reached)
‚åü \\
 
Running this computation for $7$, we do indeed get to $1$ in sixteen
steps. But then we're stuck in a so-called "trivial loop," i.e., at
Step 16 we get $1$ === then by strictly following the recipe we keep
going: $1$ is odd, hence, multiply by $3$ and add $1$ getting $4$,
which is even, then divide by $2$, which is $2$, then divide by $2$
and we're back at $1$ --- over and over. If we've written a program to
run the Collatz, and our software is not told to break the trivial
loop, the reducing will go on indefinitely.

‚•§ /Knowing if a program ends properly or just keep running is a huge
issue in the computer world./

Is something computable? The so-called **[[https://en.wikipedia.org/wiki/Halting_problem][Halting problem]]** is about
determining whether a piece of software will finish running or
continue to run forever. However, it has been proven that the halting
problem is __undecidable__, meaning that /no general way exists that
solves the halting problem for all possible programs and their
input/. This demonstrates that we can define some things
mathematically but not actually compute them. A lot more later.[fn:8]

Mathematicians have not been able to /prove/ the Collatz conjecture
always terminates at $1$ for all $n$, therefore, we cannot know for
certain whether any given number can be reduced to $1$, or whether the
calculations just keep running forever. This is a good example of why
proving conjectures, statements is crucial. Now, could a computer
simply keep trying ever bigger numbers? Yes, but that's what's known
as applying "brute force," i.e., not really desirable. With the
Collatz conjecture they have indeed applied brute force---out to $2.36
\times 10^{21}$, a colossal number. However, in the real world of math and
computer science we seek known, provable outcomes. This means we need
tools to analyze computations, even prove[fn:9] what a computation
does. Ironically, the whole question of what is computable and what is
not began in earnest, at least theoretically, on paper, back in the
1930s, /long before a single electronic computing device actually
existed/.[fn:10]

** Algebra built from axioms

Tom Apostol in his /[[https://en.wikipedia.org/wiki/Calculus_(Apostol_books)][Calculus...]]/[fn:11] textbook starts with a higher
math-style, foundation-laying formalism for Calculus,[fn:12]
essentially laying out the basics of everything we've taken for
granted for years, e.g., simple adding and multiplying. His language
is dense and thick, but we'll dissect it piece-by-piece. This is how
he starts

‚á≤@@html:<font color = "#0d3db3">&nbsp;@@Along with the set
$\mathbb{R}$ of **real numbers**, we assume the existence of two
operations called **addition** and **multiplication**, such that for
every pair of real numbers $x$ and $y$ we can form the **sum** of $x$
and $y$, which is another real number denoted by $x + y$, and the
**product** of $x$ and $y$, denoted by $xy$ or by $x \cdot y$, which is
also another real number. It is assumed that the sum $x + y$ and the
product $xy$ are /uniquely/ determined by $x$ and $y$. In other words,
given $x$ and $y$, there is exactly one real number $x + y$ and
exactly one real number $xy$. We attach no special meanings to the
symbols $+$ and $\cdot$ other than those contained in the axioms.
@@html:</font>@@

...then he gives the /Properties of Arithmetic/ you saw above, which
he calls **field axioms**. In essence, he's putting together a /tool
bag/ starting with objects such as the real numbers, addition and
multiplication. Notice, however, he doesn't tell us /how/ to add or
multiply; instead, we're getting a purely symbolic and, thus, purely
operational treatment. And when he "attaches no special meaning" to
the /symbols/ $+$ and $\cdot$, he's all but saying forget what $+$ and $\cdot$
even are. Indeed, this is about symbolic manipulation and /only/
symbolic manipulation. We'll take a deeper dive into mathematical
logic soon, but for now think back on when you were introduced to
/[[https://en.wikipedia.org/wiki/Syllogism][syllogisms]]/. For example

#+begin_example
  All men are mortal.
  All Greeks are men.
‚à¥ All Greeks are mortal.
#+end_example

can be generalized by letter symbols as[fn:13]

#+begin_example
Major premise: All M are P.
Minor premise: All S are M.
Conclusion:    All S are P.
#+end_example

Hmm. Doesn't this whole business seem tedious, over-the-top? Can't we
just /do/ adding and multiplication and not get so lost in the weeds?
It's almost like he wants to explain arithmetic to a robot or a
computer. Hold that thought... Meanwhile, let's take an extended /ex
situ/ math holiday to figure out how deep the idea of /one unique
answer/ to addition and multiplication can go. Click on the following
link: [[file:ComputingInContext1AuxExistUnique.html][Existence and uniqueness]].

...yes, nice, long math holidays can be so refreshing... Back to
Apostol who boldly states

#+begin_quote
All the workings of elementary algebra can be **deduced** from these
six [Properties of Arithmetic] axioms.
#+end_quote

To be sure, we're attempting to /deduce/ algebra from axioms---all in
this highly symbolic, non-intuitive proof way. But again, why all this
formalistic stuff? One way to answer this is to try to think about
computers and how math might be done on them. Computers run
**[[https://en.wikipedia.org/wiki/Algorithm][algorithms]]** expressed in computer code written in computer
programming languages. We'll explore algorithms as we go, but suffice
it to say, when a computer computes it uses carefully constructed,
mathematically-based algorithms for which all the logical consequences
have been accounted. Here's what [[https://gemini.google.com/app][Google Gemini AI]] had to say

#+begin_quote
The algorithmic way of thinking is a problem-solving approach focused
on breaking down complex challenges into a sequence of clear, logical,
and finite steps, much like a cooking recipe, allowing for systematic
solutions that are precise, repeatable, and can be automated for both
humans and machines, moving from general tasks like tying shoes to
complex scientific computations by defining rules and procedures for
achieving desired outcomes.
#+end_quote

Gemini goes on to list four steps of algorithmic thinking

- /Decomposition/: Breaking a big problem into smaller, manageable parts.
- /Pattern Recognition/: Identifying similarities or trends in the data or problem.
- /Abstraction/: Focusing on essential details while ignoring irrelevant ones.
- /Algorithm Design/: Creating step-by-step instructions to solve the problem. 

Now, here's the million-dollar question: Can we turn the /Properties/,
the field axioms, as well as Apostol's theory-heavy explanation above of sum
and product in the real numbers into lines of computer code, into
algorithms with which we can do algebra? Something like creating a
cookbook of recipes, no? Again, hold that thought, because it is
central to everything we're doing here. /The connection between
axiomatic math and the algorithm-based computer world is vast and,
dare we say, magical./ Simply put, it comes down to the question, Can
we /operationalize/ something so the computer can do it? You learned
algebra more like a circus animal learns circus tricks, but a computer
cannot.[fn:14]

Math did not have this level of rigorous formalism until the
late-1800s when the world of mathematics began asking itself, Can we
really just /assume/, take for granted the operations we do in
arithmetic? The answer they came to was no. And so was birthed
mathematics based on setting up axioms then postulating and proving a
body of theorems. And throughout modern STEM we see this model of
starting with a set of basic, given facts, /axioms/, then using them
to derive theorems, i.e., a body of working parts.

In the algebra example above where we simplified $5x - 12 = 3(x + 2)$
to find $x$, each step was justified, matched with one of the
/Properties of Equality/ (PoE) arguments. But then Apostol counts some
of these PoEs as theorems themselves and, as such, must first be
proven. His first **Theorem 1.1**, the /Cancellation law for
addition/, (compare with /Addition property of equality/ above)
states[fn:15]

‚á≤@@html:<font color = "#0d3db3">@@ If $a + b = a + c$, then $b = c$. In
particular, this shows that the number $0$ of Axiom 4 is unique.
@@html:</font>@@

Perhaps you intuitively get what is meant --- we "lose" the $a$ ---
but you may still wonder how this definition, as it was just given,
creates, enables the cancelling of "like things" on each side of an
equation. Also, he ties in his Axiom 4 saying the uniqueness of $0$ an
issue. Before we look at his proof let's keep in mind he's only saying

‚•§ /If you have a common term added to both sides of an equality, you
are logically permitted to remove it./

Then comes his proof:[fn:16]

‚á≤@@html:<font color = "#0d3db3">@@ Let us assume $a + b = a + c$ is
true. By Axiom 5 (see /Existence of negatives/ above), there exists a
number $y$ such that $y + a = 0$. Since sums are uniquely determined,
we have $y + (a + b) = y + (a + c)$. Using the associative law, we
obtain $(y + a) + b = (y + a) + c$ or $0 + b = 0 + c$. But by Axiom 4
(see /Existence of identity elements/ above) we have $0 + b = b$ and
$0 + c = c$, so that $b = c\:$. Notice that this theorem shows that
there is only one number having the property of $0$ in Axiom 4. In
fact, if $0$ and $0^\prime$ both have this property, then $0 + 0^\prime = 0$ and
$0 + 0 = 0$. Hence $0 + 0^\prime = 0 + 0$ and, by the cancellation law, $0
= 0^\prime$.
@@html:</font>@@

The idea of "crossing/cancelling out" on both sides, or, "whatever you
do to one side, you have to do the same to the other side" or
"balancing the equation" has just been formally explained and
proved---even if at first glance it's hard to grasp how. Again,
Apostol calls it a theorem, his /Cancellation law for addition/, he
then proves it, and now we may use it.[fn:17] And so his Theorem 1.1
ropes together /Axiom 5, Existence of negatives/, /Axiom 2,
Associativity/, and /Axiom 4, Existence of identity elements/. But
what's going on with "...only one number having the property of $0$",
and then in Theorem 1.1 itself, "...this shows that the number $0$ of
Axiom 4 is unique." Quick unpack follows:

- Suppose (for whatever reasons) there are two zeros: $0$ and a second
  "rogue" zero $z$.
- By definition of the identity element: $a + 0 = a$ and $a + z = a$.
- $\therefore \; a + 0 = a + z$.
- Applying our newly-minted Theorem 1.1 we may cancel $a$ from
  both sides...
- ...resulting in $0 = z$

...which is an absurdity, which then proves[fn:18] it is impossible to
have more than one $0$ in our "system".
  
Good. Now, let's look at this issue from another angle by asking an AI
(in this case [[https://grok.com/][Grok]]) to explain balancing an equation in a similarly
axiomatic-based way. Some of this repeats the above discussion; other
things will be different, i.e., a new perspective, a different
angle. Peruse the following sections and their discussions. New
and interesting concepts are discussed.

*** Leibniz and the substitution principle

We can narrow down algebra's equation balancing trick by using a
universal axiom and a principle developed originally by Gottfried
Wilhelm Leibniz.[fn:19]

@@html:<font color = "#0d3db3">@@
- **Reflexivity**: For any element \(a\), \(a = a\) or \( \forall x \, (x
  = x) \).
- **Substitution Principle**: If \(a = b\), and \(P(x)\) is any
  property or predicate involving \(x\), then \(P(a)\) holds if and
  only if \(P(b)\) holds.
@@html:</font>@@

First, let's nail some facts down about /reflexivity/, namely, that it
is considered in higher math to be a /logical axiom/, i.e., it is
/universally/ true, no matter what we're talking about. The opposite
would be non-logical axioms, i.e., truths that are limited to certain
situations. For example, $x ‚äï y = y ‚äï x$ or /commutativity/ is not
/universally/ true throughout mathematics.[fn:20]

In lower math, /equality/ often just means "same value." In math
logic, $\forall \;x (x = x)$ helps better establish the concept of
**identity**, which is stronger than just /equivalence/. If we did not
assume this axiom, we could technically build a weird logical model
where an object is not the same as itself all the time. For example,
observing an object $A$ at time t_{1} and observing the object at time t_{2}
are considered two different things.[fn:21] The $\forall x \, (x = x)$ keeps
sanity by insisting object $x$ is eternally /identical/ to itself.

From another angle, reflexivity also forces the whole concept of
equals to exist in the first place. Consider these "weaker" uses of
the $=$ sign. They all have the /conditional/ "If...then" structure:

- **Symmetry**

They're all presupposing equals. One logical entailment can be
introduced by the /Empty Club/ analogy.


@@html:<font color = "#0d3db3">@@ ‚á≤ **The Axioms of Equality (redux)**

Equality ($=$) is a **[[https://en.wikipedia.org/wiki/Relation_(mathematics)][relation]]** that satisfies certain logical
axioms:
- **Reflexivity**: For any element \(a\), \(a = a\) or \( \forall x \, (x
  = x) \).
- **Symmetry**: If \(a = b\), then \(b = a\) or \( \forall x \, \forall y \, (x =
  y \leftrightarrow y = x) \).
- **Transitivity**: If \(a = b\) and \(b = c\), then \(a = c\) or \( \forall
  x \, \forall y \, \forall z \, ((x = y \land y = z) \to x = z) \).
- **Substitution Principle**: If \(a = b\), and \(P(x)\) is any
  property or predicate involving \(x\), then \(P(a)\) holds if and
  only if \(P(b)\) holds. 
@@html:</font>@@

Grok's explanation is telling us that balancing an algebra equation
can be grounded in these so-called /Axioms of Equality/, the first
three we saw above. However, the /Substitution Principle/ is new and
is supposedly key. Whether obvious right now or not, it is what allows
us to replace equals with equals in expressions. Obviously this is
different from Apostol's approach.



To start let's interpret a bit, then justify: If $a = b$ then $a$ can
/replace/ (substitute in for) $b$ (and vice versa) in any expression
without changing the value of that expression. Think of $a$ and $b$ as
two different nametags for the exact same object. If you have a box
labeled "Box A" and a box labeled "Box B", and they contain the exact
same weight, you can swap them at any time, and the total weight of a
larger stack won't change. But how are either Apostol's /Theorem 1.1:
Cancellation law for addition/ or this new /Substitution Principle/
giving us equation balancing? Let's now reconcile Apostol with the
/Substitution Principle/.[fn:22] So we'll start again with Apostol's
Theorem 1.1: @@html:<font color = "#0d3db3">@@If $a + b = a + c$, then
$b = c$.@@html:</font>@@

1. Consider it given: $a + b = a + c$, then proceed...
2. The substitution step: Because the expression $(a + b)$ is
   identical in value to $(a + c)$, there is "same on both sides",
   thus, we can "/do/ the same thing" to both. /And so we choose to
   **add** $(-a)$ to the left of both sides/.
   - Leibniz/Substitution justification: If $x = y$, then $(-a) + x =
     (-a) + y$, or, in our case, $(‚àía) + (a + b) = (‚àía) + (a + c)$
3. Using the Associative properties (Apostol's Axiom 2 above) we regroup:
   $((‚àía) + a) + b = ((‚àía) + a) + c$.
4. Applying the Existence of negatives (Apostol's Axiom 5 above) we know
   $(-a) + a = 0$ leaving $0 + b = 0 + c$.
5. Finally, the Existence of identity elements (Apostol's Axiom 4
   above) promises that adding $0$ changes nothing resulting in $b =
   c$.

In plain language we may "cancel," "get rid of" $a$ by /adding/ its
negative $(-a)$.
     
Since Grok is relying on its above-stated /Axioms of Equality/ for
equation balancing, let's say something about the first three axioms.

Those three axioms before the /Substitution Principle/ are making
exacting points about just what equality is. In higher math the
concept of equality, of /sameness/ goes much deeper and is more
precise and nuanced than our everyday intuitive understanding of
/equals/---as we can see with Leibniz's very theoretical
Indiscernibility of Identicals. In one formal definition, equality is
called an /equivalence relation/ on numbers.[fn:23]

Grok's answer is compelling us once again to tackle some higher math
based in set theory and [[https://en.wikipedia.org/wiki/Mathematical_logic][mathematical logic]]. We will weave both sets
and logic into our presentations as we go.

One part of math logic is **[[https://en.wikipedia.org/wiki/First-order_logic][first-order logic]]** or FOL. Again, nothing
in depth now, but we'll need a quick intro to FOL to understand what's
being said about /predicates/ in the /Substitution Principle/ above.

Grok's first axiom is **reflexivity**, which might seem
/vacuous/[fn:24] and unnecessary to the layperson. Of course any given
thing is the same, /equal/ to itself. But consider how reflexivity is
/a relation each thing bears to itself and nothing else/. We are
stating this explicitly to be thorough. Believe it or not, having
reflexivity as an axiom is necessary in certain situations.

Next, **symmetry** is allowing expressions to swap sides of an
equation, i.e., what's on the left can go to the right and vice-versa,
and their equality is not changed. Again, you were just conditioned
to accept this, but here it is formally stated.

And now let's take an /in situ/ math holiday to look into FOL so we
can understand what's going on with predicates and why the FOL
statement $\forall x \, (x = x)$ firms up what we were saying in /Uniqueness
and existence/ about $=$ in logic beingis just another version of $a = b$.[fn:25]

The **Substitution Principle** introduces the notion of a
**predicate** $P(x)$. To get started, let's reword the /Substitution
Principle/ for just functions: If \(a = b\), then for any /function/
\(f\), \(f(a) = f(b)\). "Doing the same thing" to both sides means
applying the same function to both. As you might suspect at this point
the concept of a predicate is something fancier, broader-based than a
function. And you're right.



@@html:<font color = "#0d3db3">@@
‚á≤ **Algebraic structure: field axioms (redux)**

We work in a /field/ (like the real numbers \(\mathbb{R}\)), which has
two operations: addition (+) and multiplication (¬∑), satisfying axioms
such as:
- **Commutativity**: \(a + b = b + a\), \(a \cdot b = b \cdot a\).
- **Associativity**: \((a + b) + c = a + (b + c)\), similarly for multiplication.
- **Distributivity**: \(a \cdot (b + c) = a \cdot b + a \cdot c\).
- **Identity Elements**: There exists 0 such that \(a + 0 = a\), and 1 such that \(a \cdot 1 = a\).
- **Inverses**: For every \(a\), there exists \(-a\) such that \(a + (-a) = 0\); for every \(a \neq 0\), there exists \(a^{-1}\) such that \(a \cdot a^{-1} = 1\).
- **Closure**: The set is closed under + and ¬∑.

@@html:</font>@@  

These axioms define how operations behave, but equality ties them to the "balancing" rule.

*** Preserving equality through operations

Starting from an equation \(a = b\), applying the same operation to both sides preserves equality due to substitution and the field axioms. Here's how common operations are justified:

- **Adding the same value \(c\) to both sides**:  
  Consider the function \(f(x) = x + c\). By substitution, if \(a = b\), then \(f(a) = f(b)\), so \(a + c = b + c\).  
  (This uses the definition of addition as a binary operation.)

- **Subtracting the same value \(c\) from both sides**:  
  This is equivalent to adding \(-c\): \(a + (-c) = b + (-c)\), or \(a - c = b - c\).  
  Justified by the additive inverse axiom.

- **Multiplying both sides by the same value \(c\)**:  
  Function \(f(x) = x \cdot c\), so \(a \cdot c = b \cdot c\).  
  (If \(c = 0\), this preserves equality but may lose information, as both sides become 0.)

- **Dividing both sides by the same non-zero value \(c\)**:  
  Multiply by the multiplicative inverse \(c^{-1}\): \(a \cdot c^{-1} = b \cdot c^{-1}\), or \(a / c = b / c\).  
  The axiom requires \(c \neq 0\) to ensure \(c^{-1}\) exists.

More complex operations (e.g., squaring both sides) follow similarly: Define \(f(x) = x^2\) (which is \(x \cdot x\)), and apply substitution. However, note that some functions (like squaring) are not one-to-one, so they may introduce extraneous solutions, but equality is still preserved in the forward direction.

*** Why this maintains balance

The rule isn't arbitrary‚Äîit's a consequence of equality being an equivalence relation and operations being well-defined functions. If you only apply an operation to one side, you're no longer comparing equal expressions; you've changed one without the other, violating substitution. Axiomatically, this ensures that solving equations (e.g., isolating variables) is logically sound and leads to equivalent statements.
@@html:</font>@@





The multi-volume set [[https://en.wikipedia.org/wiki/Euclidean_geometry#Axioms][Euclid's Elements]] (EE) from ca. 300 BC Greece is
generally accepted as being the first example of mathematics built on
*[[https://en.wikipedia.org/wiki/Axiom][axioms]]* and built up through proven *[[https://en.wikipedia.org/wiki/Theorem][theorems]]* based on those
axioms[fn:26] ... and EE has been, in one form or another, a part of
the Western world's math curricula ever since. It was once a staple of
high school math, /though often dreaded/, as it was the student's
first exposure to the strange idea of /proving/ math instead of just
doing math. Proving something in math requires a deep grasp of a
topic, which is completely different from the typical style of "when
you see this, do this" group behavior conditioning. Starting with
[[https://en.wikipedia.org/wiki/Euclidean_geometry#Axioms][Euclidean Geometry]], the student is asked to see a geometric
situation[fn:27] and understand why something about it is /necessarily/
(proven to be) true. The proof of, e.g., the /pons asinorum/ is a
step-by-step establishing of undeniable facts based on Euclid's
axioms, his /Postulates/ and /Notions/. Again, this was the beginning
of modern mathematics where a set of axioms could be called on to
build /theorems/, i.e., additional true things about the
situation. 

Axioms can go by many names, e.g., basic truths, givens, primitives. Here are
Euclid's *Five Postulates*


- P1 :: A straight line may be drawn between any two points.
- P2 :: Any terminated straight line may be extended indefinitely.
- P3 :: A circle may be drawn with any given point as center and any
  given radius.
- P4 :: All right angles are equal.
- P5 :: Through a given point $P$ not on a line $L$, there is one and
  only one line in the plane of $P$ and $L$ which does not meet $L$.

The last postulate is a more modern rewording of the original, meaning
/truly parallel lines may exist that never cross/. To these first
five, Euclid added another five basic truths, his *Common Notions*

- N1 :: Things which are equal to the same thing are also equal to
  each other.
- N2 :: If equals are added to equals, the wholes (results) are equal.
- N3 :: If equals are subtracted from equals, the remainders are equal.
- N4 :: Things which coincide with one another are equal to one another.
- N5 :: The whole is greater than a part of the whole.

With these ten axiomatic statements begins much geometry, logic,
number and set theory. That is to say, great amounts of /[[https://en.wikipedia.org/wiki/Propositional_calculus#Inference_rules][implication]]/,
"if this then that" can be derived from these ten accepted
givens.[fn:28] For example, the last notion, N5, seems to say something
almost silly and trivial. But as [[http://aleph0.clarku.edu/~djoyce/elements/bookI/cn.html][this treatment]] explains, N5 with its
odd wording starts whith the basic concept of one thing being /larger/
than another, i.e., $A>B$. Here's David E. Joyce's, professor of
mathematics at Clark University, explanation

#+begin_quote
To say one magnitude $B$ is a *part* of another $A$ could be taken as
saying that $A$ is the sum of $B$ and $C$ for some third magnitude
$C$, the remainder. Symbolically, $A > B$ means that there is some $C$
such that $A = B + C$.
#+end_quote

Again, to the novice this may look like nit-picking, but real math
comes from having these basic foundational givens---upon which new,
useful math is solidly, logically built. Higher math[fn:29] is the
realm of axiomatic math where proofs of /theorems/ are the
staple. Higher math is where the student becomes a real mathematician
who looks under the hood and learns what makes things really tick.

Let's consider a more serious-looking set of axioms, again, taken from
course material by Professor Joyce, then a proof of a theorem based,
i.e., relying on these axioms[fn:30]

1. Vector addition is /commutative/: $\mathbf{v} + \mathbf{w} = \mathbf{w} + \mathbf{v}$.
2. Vector addition is /associative/: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$.
3. There is a vector, denoted $\mathbf{0}$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v} = \mathbf{0} + \mathbf{v}$.
4. For each $\mathbf{v}$, there is another vector $\mathbf{‚àív}$ such that $\mathbf{v} + (\mathbf{‚àív}) =
   0$.
5. Scalar multiplication /distributes/ over vector addition: $c(\mathbf{v} + \mathbf{w}) =
   c\mathbf{v} + c\mathbf{w}$.
6. Scalar multiplication /distributes/ over real addition: $(c + d)\mathbf{v} = c\mathbf{v} + d\mathbf{v}$
7. Multiplication and scalar multiplication /associate/: $c(d\mathbf{v}) =
   (cd)\mathbf{v}$.
8. The number $1$ acts as /identity/ for scalar multiplication: $1\mathbf{v}
   = \mathbf{v}$.

And now we'll prove a very basic theorem relying on these axiom truths

@@html:<font color = "#650d1c">@@
Theorem: $0\mathbf{v} = 0$
@@html:</font>@@

...or zero times a vector $\mathbf{v}$ is zero.

‚åú\\
@@html:<font color = "#650d1c">@@Proof: @@html:</font>@@ Since $0 + 0
= 0$, therefore we can say $(0 + 0)\mathbf{v} = 0\mathbf{v}$. By axiom *6*, that implies
$0\mathbf{v} + 0\mathbf{v} = 0\mathbf{v}$. If we could subtract $0\mathbf{v}$ from each side, we‚Äôd be
done, but subtraction isn‚Äôt yet defined. Still, we can add the
negation of $0v$ to each side which should accomplish about the same
thing. Thus,

\begin{align*}
(0\mathbf{v} + 0\mathbf{v}) + (‚àí0\mathbf{v}) = 0\mathbf{v} + (‚àí0\mathbf{v}).
\end{align*}

Next, we can associate the parentheses differently by axiom *2* to get

\begin{align*}
0\mathbf{v} + (0\mathbf{v} + (‚àí0\mathbf{v})) = 0\mathbf{v} + (‚àí0\mathbf{v}).
\end{align*}

That equation simplifies by axiom *4* to $0\mathbf{v} + 0 = 0$, and by
axiom *3*, that further simplifies to $0\mathbf{v} = 0$ which is what
was to be proved. *Q.E.D.*[fn:31] \\
‚åü

Got it? No worries (yet) if you didn't. The point is to understand
that this so-call higher math thing wants to be built exclusively on
axioms. But yes, this is a strange world for the beginner so used to
the usual "get stuff calculated" math. For example, in beginner
algebra we never needed to /show/ with a proof that an entity times
zero is zero. So yes, here we have a thorough /proof/ of something
that we normally would have taken for granted. But again, this is the
world of higher math where math becomes more of a philosophy than just
a set of tools.[fn:32]

** Axiomatic Scheme

And now with no further ado we will list out TLS's version of
"axiomatic givens" starting with /The Ten Commandments/ below[fn:33]

- *First Commandment*: When recurring on a list of atoms, ~lat~, ask two
  questions about it ~(null? lat)~ and ~else~; when recurring on a
  number, ~n~, ask two questions about it: ~(zero? n)~ and ~else~;
  when recurring on a list of S-expressions, ~l~, ask three question
  about it: ~(null?  l)~, ~(atom? ( car l))~, and ~else~.
- *Second Commandment*: Use ~cons~ to build lists.
- *Third Commandment*: When building a list, describe the first typical
  element, and then ~cons~ it onto the natural recursion.
- *Fourth Commandment*: Always change at least one argument while
  recurring. When recurring on a list of atoms, ~lat~, use ~(cdr
  lat)~. When recurring on a number, ~n~, use ~(sub1 n)~. And when
  recurring on a list of S-expressions, ~l~, use ~(car l)~ and ~(cdr
  l)~ if neither ~(null? l)~ nor ~(atom? (car l))~ are true. [One
  argument] must be changed to be closer to termination. The changing
  argument must be tested in the termination condition: When using
  ~cdr~, test termination with ~null?~ and when using ~sub1~, test
  termination with ~zero?~.
- *Fifth Commandment*: When building a value with $+$, always use $0$
  for the value of the terminating line, for adding $0$ does not
  change the value of an addition.
- *Sixth Commandment*: Simplify only after the function is correct.
- *Seventh Commandment*: Recur on the /subparts/ that are of the same
  nature: on the sublists of a list; on the subexpressions of an
  arithmetic expression.
- *Eighth Commandment*: Use help functions to abstract from
  representations.
- *Ninth Commandment*: Abstract common patterns with a new function.
- *Tenth Commandment*: Build functions to collect more than one value at
  a time.

But wait there's more. Next are the /The Five Rules/

- *The Law of ~car~*: The primitive ~car~ is defined only for nonempty
  lists.
- *The Law of ~cdr~*: The primitive ~cdr~ is defined only for nonempty
  lists. The ~cdr~ of any nonempty list is always another list.
- *The Law of ~cons~*: The primitive ~cons~ takes two arguments. The
  second argument to ~cons~ must be a list. The result is a list.
- *The Law of ~null?~*: The primitive ~null?~ is defined only for lists.
- *The Law of ~eq?~*: The primitive ~eq?~ takes two arguments. Each must
  be a non-numeric atom.

Not many beginners can take these fifteen would-be axioms and know
exactly how to write a Scheme program.[fn:34] But then if you've read
and understood a textbook such as /[[https://cseweb.ucsd.edu/~gill/BWLectSite/Resources/LDGbookCOV.pdf][Lists, Decisions and Graphs...]]/ by
Edward Bender and S. Gill Williamson, you might catch on that these
"fake" axioms are attempting is to establish some sort of
omnibus[fn:35] /list processing/ machinery. In fact Scheme's
predecessor, *Lisp*, is an acronym for "list processing," because this
is essentially what Lisp and Scheme do, i.e., they do computing tasks
with, on lists. Really?! So in Lisp and Scheme we have a complete
working programming language as just a bunch of lists stored in a
computer file that some sort of evaluator processes and gives results?
How does that work? Read on.

** List basics

At this point we take a theoretical look at what a list is, as the
humble list is the most basic data structure in programming. But then
we all know in the everyday sense what a list is, right? A grocery
list is a good example

- eggs
- milk
- flour
- potatoes
- butter

But we have a problem---at least with the math world. That's because
in higher math there are also **sets**. So what is the difference
between a list and a set? To the layman they might seem
interchangeable. /[[https://en.wikipedia.org/wiki/Set_theory][Set theory]]/ is the fundamental starting point of
higher math, as well as CS's Discrete Math.[fn:36] And so many books
on set theory and discrete math start out with the deceptively simple
and informal statement

#+begin_quote
@@html:<font color = "#0d3db3"> <i>@@ A **set** is collections of
things ... objects ... stuff. @@html:</font> </i>@@
#+end_quote

...which is kind of what a list is too. But to CS and higher math they
are two different entities. What's the difference? First, our grocery
/list/ is not technically a list because, again, in the CS-higher math
world,@@html:<font color = "#0d3db3">@@ /the *elements* (members) of
a list have a specific order/ @@html:</font>@@. But then our grocery
list above could be in any order, doesn't really matter as long as you
come home with the those groceries...

Plainly put, lists are defined as a collection of things in a specific
order. Think of a /string/ of letters like you're reading right now. A
string of alphanumeric characters can be considered a list because
/the order of the individual alphanumeric characters (and spaces in
between) is everything/. We typically call strings of letters and
spaces /sentences/ ... where all the sentences in English are made up
of differing combinations of the twenty-six letters (times two if we
have both upper- and lower-case), as well as the ten numerals, all the
punctuation, and the space character. You can't be placing individual
characters in any order and still have sensible sentences. So is our
"grocery list" really a /set/ of groceries?

Actually no. We should not call a grocery list a /set of groceries/
either because even though order doesn't matter, sets allow
duplicates. They do. In set theory there is the idea of the
/[[https://en.wikipedia.org/wiki/Cardinality][cardinality]]/ of a set, i.e., what is the count of the /unique/
elements of the set, /not/ counting duplicates.[fn:37]

‚åú\\
ùñüùï≠: The set $A = \{1,1,1,2,2\}$ is /identical/ to set $B = \{1, 2\}$
because they have the same cardinality, or $|\,A\,| = |\,B\,| =
2$.[fn:38] \\
‚åü\\

This can be explained in a few ways. The simplest is to say the
members, the elements of a set /belong/ to that set. So for example,
if I /belong/ to a club, it doesn't make sense to have me
/belong-belong/ or belong twice to the club.[fn:39] In the example
above, $1$ /belongs/ to $A$, so it doesn't need to belong again.

Splitting hairs? Yes, well, set theory is a very fundamental thing in
higher math, thus, very precisely defined based on axioms. For
example, set theory has the /[[https://en.wikipedia.org/wiki/Axiom_of_extensionality][Axiom of Extensionality]]/ which states
@@html:<font color = "#0d3db3">@@ two sets are equal if they both have
the same members@@html:</font>@@. In our example $A$ and $B$ do indeed
have the same members, namely, $1$ and $2$ ---and it doesn't matter,
it doesn't count that $A$ repeats these members. Let's look at another
example

‚åú\\
ùñüùï≠: Given the *set* $A = \{\text{good, so-so, bad}\}$ and a defined
(named) *list*

#+name: 7f8774e8-5919-413d-8134-dfd3652d7b39
#+begin_src scheme :eval never :exports code
(define week3 '(bad so-so so-so so-so good))
#+end_src
‚åü\\

the set $A$ contains the three possible moods a person might be in;
while ~week3~ is the name we've told Scheme to call a list containing
our daily workweek moods.[fn:40] \\

The order of set $A$ doesn't matter (and we'd ignore, say, a second
$good$), but the order of the list defined as ~week3~ is important
because we don't want to mix up how we felt on which of the five work
days. Also, it makes perfect sense for a list to have "repeats" since
we're evaluating many days in succession, and each day can be one of
the three choices. But then notice one more thing about a set: When we
built the list ~week3~ we had to choose for each day ~good~ /or/ ~bad~
/or/ ~so-so~. Or did we? What would the list ~week3~ look like if we
could have more than just one mood during a day? How about

#+name: 5e0271e8-a0e4-430f-b594-f11e30c11403
#+begin_src scheme :eval never :exports code
(define week3 '(bad bad so-so (good so-so bad) (good so-so)))
#+end_src

Remember Venn diagrams? An /or/ situation can be shown with a **[[https://en.wikipedia.org/wiki/Union_(set_theory)][union]]**
of circles.[fn:41] Let's make a formal definition of a union:[fn:42]

@@html:<font color = "#0d3db3">@@ The union of two sets $A$ and $B$
(denoted by $A \cup B$) is the set of elements which are in $A$, in $B$,
*or* in both $A$ and $B$.  @@html:</font>@@[fn:43]

Now, for our set $A = \{\text{good, so-so, bad}\}$, if we are using
each element just once per the day slot of ~week3~ we could imagine
$A$ as made up of the individual /singleton/ sets, i.e., sets having
just one element

\begin{align*}
A_G &= \{\text{good}\} \\
A_{S} &= \{\text{so-so}\} \\
A_B &= \{\text{bad}\}
\end{align*}

and so a union $\{A_G \cup A_{S} \cup A_B \}$ will get us back to our original
$A = \{\text{good, so-so, bad}\}$. $A_G$, $A_{S}$, and $A_B$ are
**[[https://en.wikipedia.org/wiki/Disjoint_sets][disjoint sets]]** since they have no elements in common when we
union-ize them. And so if we made a Venn diagram, there would be no
overlap between any of them. Why are we couching our daily mood
choices in the language of set theory? Because we will eventually move
on to another language, namely, [[https://en.wikipedia.org/wiki/Haskell][Haskell]], where a certain class of data
types emulate just this sort of **[[https://en.wikipedia.org/wiki/Logical_disjunction][logical or]]**, i.e., the **sum
type**. While Scheme is usually not directly explained in terms of
higher math, the Haskell family of languages (Haskell, [[https://en.wikipedia.org/wiki/Standard_ML][Standard ML]],
[[https://en.wikipedia.org/wiki/OCaml][OCaml]], [[https://en.wikipedia.org/wiki/F_Sharp_(programming_language)][F#]]) are. Again, lots more about data representations of
disjoint sets, logical conjunction and disjunction when we look into
Haskell in the future.

‚åú\\
ùñüùï≠: Consider three common alphabets as sets---the Greek, Latin, and
Cyrillic alphabets

\begin{align*}
G &= \{Œë, Œí, Œì, Œî, Œï, Œñ, Œó, Œò, Œô, Œö, Œõ, Œú, Œù, Œû, Œü, Œ†, Œ°, Œ£, Œ§, Œ•, Œ¶, Œß, Œ®, Œ©\} \\
L &= \{A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z\} \\
C &= \{A, –ë, –í, –ì, –î, E, –Å, –ñ, –ó, –ò, –ô, K, –õ, –ú, –ù, –û, –ü, –†, –°, –¢, –£, –§, –•, –¶, –ß, –®, –™, \\
&  –´, –¨, –≠, –Æ, –Ø\}
\end{align*}

Now, let's make one big union-ed together set $A$ out of the
individual sets $G$, $L$, and $C$

\begin{align*}
A &= \{ Œë, Œí, Œì, Œî, Œï, Œñ, Œó, Œò, Œô, Œö, Œõ, Œú, Œù, Œû, Œü, Œ†, Œ°, Œ£, Œ§, Œ•, \\
& Œ¶, Œß, Œ®, Œ©, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, \\
& S, T, U,  V, W, X, Y, Z, A, –ë, –í, –ì, –î, E, –Å, –ñ, –ó, –ò, –ô, K, –õ, –ú, –ù, \\
& –û, –ü, –†, –°, –¢, –£, –§, –•, –¶, –ß, –®, –™, –´, –¨, –≠, –Æ, –Ø\}
\end{align*}

Below we see a Venn diagram for $A$, i.e., circles in which the
/three/ sets of alphabetic characters are contained. We can see how
the intersecting circumference lines also show the what /any two/
alphabets share, and then in the very center what they /all/ share

#+begin_figure
#+CAPTION: Three alphabets, their /union/ and /intersections/ (Wikipedia [[https://en.wikipedia.org/wiki/Venn_diagram][Venn diagram]])
[[file:images/Venn_diagram_gr_la_ru.png]]
#+end_figure
‚åü\\

Of course for visual purposes not /all/ the letters could be crowded
into the diagram above, but we can see how the Venn diagram with its
overlapping circles trick is beating duplicates. Visually we can see
this happening, but is there a formal explanation? Short answer: yes.

Now let's look at a formal definition of union in so-called
**set-builder notation**[fn:44]

\begin{align}
A \cup B = \{x : x \in A \lor x \in B  \}
\end{align}

This expression says the **union** of $A$ and $B$ equals the set of
elements denoted by $x$ such that an $x$ can be an element from[fn:45]
$A$ *or* an element from $B$.[fn:46] But wait, *or* doesn't quite
sound right in English if we mean to union-ize two or more things,
does it?  To bring two or more sets together as one---their unique and
shared element alike---don't we mean to say, e.g., @@html:<font color
= "#0d3db3">@@ /the union of $A$ and $B$ takes all the elements from
both $A$ *and* $B$/?@@html:</font>@@ Yes, but set theory prefers to
look at, e.g., the Venn diagram of the alphabets (Figure 2 above) and
say, @@html:<font color = "#0d3db3">@@A letter /belongs/ to the union
of alphabets if it is from the Greek *or* from the Latin *or* from the
Cyrillic alphabets.@@html:</font>@@ Again, we have the idea of sets
and /belonging/ to sets.

Now, look at that bulging middle triangle region of Figure 2
containing the letters all three alphabets have in common. This is the
**[[https://en.wikipedia.org/wiki/Intersection_(set_theory)][intersection]]** of alphabet sets $G$, $L$, and $C$. In this case we
would say: @@html:<font color = "#0d3db3">@@A letter /belongs/ to the
center triangle (the intersection) if it belongs to $G$ /and/ it
belongs to $L$ /and/ it belongs to $C$@@html:</font>@@. In other
words, the letter has to belong to all three alphabets. This is more
precise, something upon which higher math always insists.

@@html:<label for="mn-demo" class="margin-toggle"></label>
<input type="checkbox" id="mn-demo" class="margin-toggle">
<span class="marginnote">@@
[[file:images/On_Off.jpg]]
\\
@@html:</span>@@

And with one final piece of the puzzle we can put to rest this whole
duplicates within sets issue. Let's explore another logic concept that
we actually encounter every day. In the modern world we're constantly
turning things on and off, e.g., a light switch with its on-off
toggle. But occasionally we see /two/ buttons---/one/ for on, /one/
for off. In the image to the right we have /two/ buttons doing on and
off instead of one. In such a case pushing the green /on/ button just
one initial time turns the device on /and any additional pushing is
disregarded/. This is the same for the red /off/ button. Push once for
off---and any further pushes mean nothing. Basically, "I heard you the
first time, you don't have to repeat yourself." This phenomenon has an
official name: **[[https://en.wikipedia.org/wiki/Idempotence][idempotence]]**, and it crops up in many real-life
places. For our set theory discussion imagine making a Venn diagram of
the union of sets $S_1 = \{5\}$ and $S_2 = \{5\}$, i.e., two different
sets containing the same single element. Wouldn't this union be just
one single circle containing the element $5$? Indeed it would. And if
we added another set $S_3 = \{5\}$ to the union, we would still have

\begin{align*}
S_1 \cup S_2 \cup S_3 = \{5\}
\end{align*}

i.e., just one circle representing $5$. In fact, we could union
together an infinite amount of **singleton**[fn:47] sets of the form
$S_n = \{5\}$ and they would produce the singleton set $\{5\}$. Now
for a bit of abstraction. Consider a **[[https://en.wikipedia.org/wiki/Binary_operation][binary operator]]**, i.e., any
math that takes two objects and does an operation on them to produce
something new. For example, addition is a binary operation, $1 + 1$ is
$2$, as is multiplication, $3 \cdot 5$ is $15$. Let's then use the
abstract symbol $\oplus$ to indicate any sort of binary operator. Now we
can establish a formal definition of idempotence \\

@@html:<font color = "#0d3db3">@@
- A **binary operation** $\oplus$ is said to be **idempotent** on a set $S$ if
  $x \oplus x = x \,\,\text{for all}\,\, x \in S$
- An element $x$ of a set $S$ is said to be **idempotent** under a
  binary operator $\oplus$ if $x \oplus x = x$
@@html:</font>@@

If multiplication is the binary operator on the set of all counting
numbers $\mathbb{N}$ then what numbers of $\mathbb{N}$[fn:48] can we
call idempotent according to the definition? Well, only $0$ and $1$
are idempotent:

\begin{align*}
0 \cdot 0 &= 0 \\
1 \cdot 1 &= 1
\end{align*}

All other numbers ($x \gt 1$) give increasingly larger squares of
themselves, hence, $0$ and $1$ are idempotent, but the binary
operation of multiplication is not.[fn:49]

Now, let's consider something that came up in a [[https://math.stackexchange.com/][StackExchange
Mathematics]] discussion [[https://math.stackexchange.com/questions/155475/1-1-1-origin-of-this-convention][here]], which is tackling our issue of duplicates
in a set by asking, why is $\{1,1\} = \{1\}$? Below we have a slight
re-wording of the preferred, green-checked answer:

@@html:<font color = "#0d3db3">@@
An unordered tuple $\{a_1,a_2,a_3,a_4\dots\}$ is defined as \\
\begin{align*}
\{x:x=a_1 \lor x=a_2 \lor x=a_3 \lor x=a_4 \lor\dots\}.
\end{align*}
By this convention,
\begin{align}
\{1,1\} = \{x:x=1 \lor x=1 \}
\end{align}
So by the /idempotency/ of $\lor$ we have (2) the same as $\{ x : x = 1
\}$, hence,
\begin{align*}
\{1,1\} = \{1\}
\end{align*}
@@html:</font>@@

Notice we are relying on the **logical-or** binary operator being
idempotent for all $x$ in the set. Generally, an **unordered n-tuple**
is a set of the form $\{a_1,a_2,\ldots, a_n\dots\}$, although a **tuple** is
usually meant to be **ordered**.[fn:50] Again, idempotency of two
identical sets is obvious when seen as Venn diagrams, but now we've
seen in exact language how they are idempotent.

By the way, **union** and **logical or** are complimentary ideas. Consider
this expression

\begin{align}
a \in X \cup Y \iff a \in X \lor a \in Y
\end{align}

What does that double-arrow in the middle mean? **if and only
if**. We'll go into $\iff$ in more exact detail later, but for now it
means @@html:<font color = "#0d3db3">@@the truth of one side of the
$\iff$ requires the other side to be true also.@@html:</font>@@ So if
$a$ is an element of the union of $X$ and $Y$ then $a$ must be an
element of $X$ /or/ it is an element of $Y$ /or/ it is an element of
/both/ $X$ and $Y$ ---and the other way around as well. Note: we avoid
writing $X \lor Y$ when considering just the sets without involving the
elements therein.

** Summing up; data and data structures

We didn't even get off the first page of TLS, but we have laid some of
the groundwork we'll need to proceed. And we can't emphasize this
enough: Computer science is a branch of applied mathematics, and we
need to get the math going alongside, parallel to the computer
stuff. So expect more new math. Yes, this sort of math takes
concentrated, deliberate thinking. We're becoming philosophers, i.e.,
we're going beyond "plug-and-chug" math.

Next, **lists**. While this lesson was an informal introduction to
lists, we'll learn more about how they are a very simple but important
data structure in computer **data management** ... but then we're
using the term data management differently then the greater lay world
does. So a quick word about the whole concept of **data** and **data structures** before we really dive in ahead. Today the term /data/ is
mainly understood to be gobs and gobs of stuff held in databases or
other storage schemes. That's not what we mean by data. For us /data/
can simply be a number returned from an algorithm. That is to say,
data is what the programming logic, the /code/ is taking in, working
on, then giving back to us. Yes, of course, this production of data
can pool and amass into great piles in vast electronic storage bins,
but, for instance, this is data

#+name: 6ce99fb1-93d3-4a72-9e01-5d9b96f3c682
#+begin_src scheme :eval never :exports code
'(bad bad so-so so-so good)
#+end_src

i.e., just a simple list surrounded in parentheses and prefaced at the
beginning by a quotation mark ("'") as is the custom in Scheme (and
Lisp). That quote at the opening parenthesis indicates this is not
code, rather, data. What we saw above

#+name: e4667fd4-1f04-4d95-ac36-ee0ee01b44ca
#+begin_src scheme :eval never :exports code
(define week3 '((bad bad so-so so-so good))
#+end_src

is therefore a mixture of both code and data---making it our first
Scheme program, even if it is just one line long! As we progress we'll
get into the weeds on just how this mixture of code and data can work
together.

#+INCLUDE: "./footer.org" :minlevel 1

* Footnotes

[fn:1] ...and this power is often jaw-dropping. The ways human
mathematics /precisely/ coincides with /exactly/ what nature is doing
is nothing short of supernatural. Prepare to have your mind blown with
Nobel laureate Eugene Wigner's /[[https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences][The Unreasonable Effectiveness of
Mathematics in the Natural Sciences]]/.

[fn:2] **axiom**: From the Greek /ax√≠≈çma/ meaning /for that which
commends itself as evident/.

[fn:3]  ùñüùï≠ is the German abbreviation for /zum Beispiel/ meaning "for
example," a nod to all the German-speaking contributors to higher
math.

[fn:4] An entire field of higher math is devoted to just this sort of
reducing and substituting. What we see with our algebra problem can be
considered in a general sense as /directed replacement/ of terms, aka
**[[https://en.wikipedia.org/wiki/Abstract_rewriting_system][term rewriting]]**. And when something can't be /simplified/ any
further, it's considered in **[[https://en.wikipedia.org/wiki/Normal_form_(abstract_rewriting)][normal form]]**. Where is formalized
rewriting important? For one with **[[https://en.wikipedia.org/wiki/Computer_algebra_system][computer algebra systems]]**, which
must know at an automated algorithmic level how to solve algebra
problems.

[fn:5] **atomic**: In a philosophical, mathematical context, the term
/atomic/ refers to the most fundamental, indivisible, or simplest
components within a given system. An atom is the most basic component
that cannot be broken down, /atomized/ any further.

[fn:6] Lots more proving of things as we go.

[fn:7] A /math holiday/ is a euphemism we use meaning to take a
detour, go down a rabbit hole in order to delve into some math topic
necessary to advance the discussion. We will take /in situ/,
i.e., on the same webpage, and /ex situ/ i.e., jump to another
webpage, math holidays. Following is an /in situ/ holiday.

[fn:8] This goes to the famous **[[https://en.wikipedia.org/wiki/Entscheidungsproblem][Entscheidungsproblem]]**: In
mathematics and computer science, the Entscheidungsproblem (German for
'decision problem' was a challenge posed by David Hilbert and Wilhelm
Ackermann in 1928. It asks for an algorithm that considers an inputted
statement and answers "yes" or "no" according to whether it is
universally valid, i.e., valid in every structure. Such an algorithm
was proven to be impossible by Alonzo Church and Alan Turing in 1936.

[fn:9] [[https://en.wikipedia.org/wiki/Mathematical_proof][Mathematical proof]]: Knowing whether something is true or false
is of great importance in computer science. In higher math proving
things completely takes over from doing calculations as you did in
Algebra and Calculus. More as we go.

[fn:10] In her /Recursive Functions in Computer Theory/, Rozsa Peter
nonchalantly states: /One always strives to feed "reasonable" programs
into the computer/... She and others such as Church and Kleene were
instrumental in /proving/ mathematically that whatever can be obtained
by the use of a computer is in the form of a "general recursive
numeric function." We'll try to get our heads around this as we go...

[fn:11] Apostol's college-level /Calculus, Volume 1: One-variable
calculus, with an introduction to linear algebra/ a a way station of
sorts between Calculus and deeper, more theoretical Analysis.

[fn:12] Both algebra and calculus are based in the realm of real
numbers, which uses the symbol $\mathbb{R}$, thus, Apostol's
clarification. He also refers to the /set/ of real numbers. That's
from **set theory**, and we'll soon take a little math holiday to
touch on it before going into a thorough treatment thereof...

[fn:13] ...although the second set of premises and conclusion below
suddenly seems plug-and-play for any ~M~, ~S~, and ~P~ we may come up
with, absurdities can arise.

[fn:14] ...or can it? AI based on neural networks and large language
models, e.g., ChatGPT, Gemini, Grok, seem to be doing just that. But
is this the final word? Most experts say no, AI must find a way to put
logic, i.e., logical entailment back into the picture.

[fn:15] One notorious fact about math is how different authors use
different terminology, different strategies to explain a common
topic. We need to get used to this and learn to contrast and compare.

[fn:16] Again, this is deep end. Be aware that Apostol's /Calculus.../
is a text meant for advanced college math students. But yes, in the
real world we need to build confidence in tackling challenging
material and gaining access to deeper truths.

[fn:17] One additional reason Apostol treats this as a formal theorem
rather than a property/axiom or just "common sense" is that
cancellation does /not/ work in all mathematical structures. For
example, in matrix math there is no general cancellation allowed. But
with real numbers, yes, this can and has been proven true.

[fn:18] See [[https://en.wikipedia.org/wiki/Reductio_ad_absurdum][Reductio ad absurdum]]., or proving something true by
assuming it false, then showing it cannot be false after all.

[fn:19] $\forall x \; (x=x)$ is a higher-math expression: The $\forall$ symbol
means /for all/. So, /for all/ $x$, $x = x$.

[fn:20] The symbol ‚äï means any operation, e.g., addition, subtraction...

[fn:21] This turns out to be a big deal with computers since the
/state/ of the internal memory changes. Also, /functional programming/
is based around state not changing, which is modeled by the math
function than can have only one output.

[fn:22] [[https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz][Leibniz]]'s Law (also called **[[https://en.wikipedia.org/wiki/Identity_of_indiscernibles][Indiscernibility of Identicals]]**)
and the Substitution Principle are essentially the same, with
Leibniz's Law stating that identical things share all properties,
which enables /substitutio salva veritate/ (Latin for /with truth
unharmed/), allowing one identical term to be swapped for another in a
statement without changing its truth value, forming the basis for
logical consistency in reasoning about identity.

[fn:23] We took a quick look at what a /relation/ is in our /Existence
and uniqueness/ math holiday.

[fn:24] In math and logic a **vacuous truth** is a statement that is
true, can't be refuted, but isn't really saying anything. For example,
the statement /All cell phones in the room are off/ would be vacuously
true if there were no cell phones in the room to begin with. In logic
vacuousness gets trickier. An example might be /When pigs fly, I'll
change my socks/. This contains a false premise, i.e., /when pigs fly/,
making the changing of socks impossible to ascertain.

[fn:25] Actually, when math logic is studied we usually begin with
**[[https://en.wikipedia.org/wiki/Propositional_logic][Propositional Logic]]**, then FOL. But here we'll just quickly touch
on FOL.

[fn:26] A *[[https://en.wikipedia.org/wiki/Conjecture][conjecture]]* is a tentative *[[https://en.wikipedia.org/wiki/Proposition][proposition]]* that may evolve to
become a theorem if proven true. Conjectures are mathematical
statements unproven but useful for consideration. Propositions are
"objects of belief," e.g., if someone believes that the sky is blue,
the /object/ of their belief is the /proposition/ that the sky is
blue.

[fn:27] For example the (in)famous /[[https://en.wikipedia.org/wiki/Pons_asinorum][pons asinorum]]/ in /Euclid's
Elements/ \\
[[file:images/Byrne_pons_asinorum.jpg]] \\
where it is /proven/ that indeed the angles opposite the equal sides
of an isosceles triangle are themselves equal.

[fn:28] In mathematical logic the idea of /implication/ is very
fundamental. Something /implies/ something else; because $A$, that
means $B$ as well. Implication goes by many names: /[[https://en.wikipedia.org/wiki/Material_conditional][material
conditional]]/, [[https://en.wikipedia.org/wiki/Modus_ponens][modus ponens]], [[https://en.wikipedia.org/wiki/Logical_consequence][logical consequence]]. We first see
implication in programming with the basic /if-then-else/ conditional
starting with our third chapter.

[fn:29] /Higher math/ in the U.S. typically means those college math
courses pursued in the junior and senior year of a Bachelors math
degree, i.e., those courses /after/ Calculus, Differential Equations,
and Linear Algebra.

[fn:30] These axioms relate to Linear Algebra.

[fn:31] ... /quod erat demonstrandum/, meaning "that which was to be
demonstrated".

[fn:32] Go ahead and take a "math holiday" by touring [[http://aleph0.clarku.edu/~djoyce/elements/bookI/cn.html][this page]] by
Professor Joyce. He is speaking from a quasi-[[https://en.wikipedia.org/wiki/Set_theory][set theory]] standpoint,
i.e., the basis of modern high math. But remember, Euclid was talking
strictly about geometric shapes. Set theory had not been invented
yet...

[fn:33] Do /not/ worry about what all this means. In time we will
explain all the Scheme programming syntax contained here. More
importantly, just realize how a programming language is actually based
on rules exactly like high math is built on axioms.

[fn:34] ...if we could, would each program we write technically be a
new *theorem*? More on that later. Specifically, we'll look into
Douglas Hofstadter's [[https://en.wikipedia.org/wiki/MU_puzzle][MU puzzle]] from his ground-breaking book [[https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach][G√∂del,
Escher, Bach]] (1979).

[fn:35] ...containing or including many items.

[fn:36] ...although it may be veiled or only partially explored.

[fn:37] We indicate the cardinality of a set $A$ by surrounding it
with vertical pipes: $|\,A\,|$ This is similar to the idea of a
number's /absolute value/ which uses the same vertical bar symbols.

[fn:38] ùñüùï≠ is German for /zum Beispiel/ or /for example/.

[fn:39] ...paying double membership fees?

[fn:40]  Notice once again the nested parentheses. To be sure,
everything in Scheme is a list.

[fn:41] A Venn diagram of two sets /or/-ed, /union/-ed together. \\
[[file:images/VennUnion.png]] \\
The lens-shaped center area represents the "overlap," i.e., what the
two sets represented as circles have in common.

[fn:42] $\cup$ is the symbol for union.

[fn:43] Could we have said, ...in $A$ *and* in $B$ *and* in both $A$
and $B$? Better not. More on why later.

[fn:44] (1) can be expanded for the union of more sets than just
two, e.g., $A \cup B \cup C \cup \ldots$.

[fn:45] The symbol $\in$ means "in" or "member of."

[fn:46] $\lor$ is the symbol for **[[https://en.wikipedia.org/wiki/Logical_disjunction][logical or]]**, aka **logical
disjunction**. Take a crack at the Wikipedia article.

[fn:47] As we saw above, a **singleton** (also known as a **unit set**
or **one-point set**) is a set with exactly one element.

[fn:48] $\mathbb{N}$ is the symbol for the /natural/ numbers, i.e., the
positive integers $1$, $2$, $3\ldots$ we use to count things.

[fn:49] In her 2023 layman's book /The Joy of Abstraction/
mathematician Eugenia Cheng notes how the operation of mixing paint
colors is basically idempotent. Why? If we add one paint to one other
paint we always get just one new paint color. That is, there is never
an additive or multiplicative increasing or intensifying, just a new
color. Now, how can we explain the green on-button above in terms of
set-theory idempotency?

[fn:50] ...such as, e.g., a pair of /Cartesian coordinates/ $(a,b)$
where $a$ is some real number on the /x-axis/ and $b$ is a real number
on the /y-axis/. Obviously, with a Cartesian coordinate system $(a,b)
\neq (b,a)$, since they'd be entirely different points on the plane. The
only exception would be if $a = b$.

[fn:53] ŒíŒπŒ≥ Œ£ŒæŒ∑ŒµŒºŒµœÅ: Big Schemer; ŒõŒπœÑœÑŒªŒµ Œ£ŒæŒ∑ŒµŒºŒµœÅ: Little Schemer in
Greek letters.

[fn:52] Go [[https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)][here]] to see the Wikipedia discussion about mathematic
formalism---although it's a bit formal...

[fn:51] Despite containing actual Scheme programs, i.e., real code, TLS
was written so that a layperson could follow along with no access to
nor knowledge of computers, e.g., even someone from the nineteenth
century such as [[https://en.wikipedia.org/wiki/Ada_Lovelace][Ada Lovelace]]!
